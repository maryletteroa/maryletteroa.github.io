---
layout: post
title: 2025 Learning Log
categories: [learning-log]
tags: [data-engineering]
pin: true
---

In keeping up with my goal to do more learning this year, I'm allotting some time outside the daily grind for learning. 

My overarching goal is to explore popular data engineering tools such as `dbt`, and cloud technologies such as Azure Fabric and Snowflake. I haven't worked in Databricks since 2021 so it'll be a good opportunity to re-learn and catch up with the new developments.

Here are some notes and impressions of how the learning is coming along so far, written in reverse chronological order.

dbt
- Access the notes here [dbt-notes]({% link _posts/2025-02-02-dbt-notes.md %})

## 8: 2025-02-02 (Sun)

### dbt - certifications and finished the course
I've finished the course today ðŸŽ‰. 

I've also moved the dbt notes into [another post]({% link _posts/2025-02-02-dbt-notes.md %}) to keep the log tidier.

The last section is an interview about the official dbt certifications. I'm still not sure about doing the certification at this point (I kind of want to get more hands-on time with the tool first) but I like what the interviewee said about doing certifications - you learn a lot more about the tool, faster compared to just using it everyday. For me, I do get a lot of gains studying for certs. If I didn't have those stock knowledge in the first place, it would have been a lot harder to think of other, better ways of approaching a problem. Mostly my qualms about doing certs is how expensive they are! :p

## 7: 2025-02-01 (Sat)

### dbt - Advance Power user dbt core, introducing dbt to the company

Today isn't as technical as the last couple of days.  I've covered more features of the Power User for dbt core extension powered with AI, and tips on introducing dbt to the company.

The course is wrapping up as well, and I only have one full section left about certifications. 

I learned about 
- Advance Power User dbt core
- Introducing dbt to the company

## 6: 2025-01-31 (Fri)

### dbt - variables, and dagster

Today I've covered dbt variables and orchestration with dagster. It was my first time setting up dagster. I actually liked dagster because the integration with dbt is tight. I was a bit overwhelmed though with all the coding at the backend to setup the orchestration. It might get easier if I take a deeper look at it. For now, it seems like a good tool to use with dbt. 

## 5: 2025-01-30 (Thu)

### dbt - great expectations, debugging, and logging

I've covered these topics today: `dbt-great-expectations`, debugging, and logging. 

The dbt-great-expectations package, though not really a port of the Python package, contains many tests that are useful for checking the model. I'm glad `ge` was adapted into dbt as it's also one of the popular data testing tool in Python.

## 4: 2025-01-29 (Wed)

### dbt - snapshots, tests, macros, packages, docs, analyses, hooks, exposures.

Today was pretty full. I've covered dbt snapshots, tests, macros, third-party packages, documentation, analyses, hooks, and exposures. Not sure how I completed all of these today but these are pretty important components of dbt. I'm amazed about the documentation support in this tool, and snapshot is another feature in which I say "where has this been all my life?" To think that SCD is that straight-forward in dbt.

## 3: 2025-01-26 (Sun)

### dbt - seeds, and source

Today, I've covered dbt seeds, and source. At first I thought why did they need to relable these CSV files and raw tables? The terminologies were a bit confusing at the start but I guess I should just get use to this.

## 2: 2025-01-25 (Sat)

### dbt - models, and materialization

I've covered dbt models, and materialization. These are pretty core topics in dbt - because dbt is all about models! 

### Books for LLM, and Polars

Here are some materials I came upon today

LLM Engineer's Handbook [[Amazon]](https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/) [[Github]](https://github.com/PacktPublishing/LLM-Engineers-Handbook)
- a resource when I get around to exploring LLMs 

Polars Cookbook [[Amazon]](https://www.amazon.com/Polars-Cookbook-practical-transform-manipulate/dp/1805121154/)
- [Polars](https://pola.rs/) is a fast alternative to [Pandas](https://pandas.pydata.org/docs/) specially for large datasets

### pandera

> `pandera` is a Union.ai open source project that provides a flexible and expressive API for performing data validation on dataframe-like objects to make data processing pipelines more readable and robust. Dataframes contain information that pandera explicitly validates at runtime. - [[Docs]](https://pandera.readthedocs.io/en/stable/)

Here's an example from the docs. It's interesting because it seems like a lighter version of Great Expectations wherein the data can be further validated using ranges and other conditions. It's powerful for dataframe validations.

```python
import pandas as pd
import pandera as pa

# data to validate
df = pd.DataFrame({
    "column1": [1, 4, 0, 10, 9],
    "column2": [-1.3, -1.4, -2.9, -10.1, -20.4],
    "column3": ["value_1", "value_2", "value_3", "value_2", "value_1"],
})

# define schema
schema = pa.DataFrameSchema({
    "column1": pa.Column(int, checks=pa.Check.le(10)),
    "column2": pa.Column(float, checks=pa.Check.lt(-1.2)),
    "column3": pa.Column(str, checks=[
        pa.Check.str_startswith("value_"),
        # define custom checks as functions that take a series as input and
        # outputs a boolean or boolean Series
        pa.Check(lambda s: s.str.split("_", expand=True).shape[1] == 2)
    ]),
})

validated_df = schema(df)
print(validated_df)
```

In [Pydantic](https://docs.pydantic.dev/latest/), something like this can also be used

```python
from pydantic import BaseModel, conint, confloat

class Product(BaseModel):
    quantity: conint(ge=1, le=100)  # Validates that quantity is between 1 and 100
    price: confloat(gt=0, le=1000)   # Validates that price is greater than 0 and less than or equal to 1000

product = Product(quantity=10, price=500)
print(product)

```

### Copilot installation, an update

The VS Code plugin that I was trying to install yesterday is working now and I am able to access chat on the side panel as well as see the prompts on screen. Not really sure what fixed it but it could be the reboot of my computer.

## 1: 2025-01-24 (Fri)

### dbt - introduction ,and setting up

I'm going through the dbt course in Udemy [The Complete dbt (Data Build Tool) Bootcamp: Zero to Hero](https://www.udemy.com/course/complete-dbt-data-build-tool-bootcamp-zero-to-hero-learn-dbt/).[^2] [^3]


[^2]: It was between this and [dbt Learn](https://www.getdbt.com/dbt-learn) platform - I might go back to that for review later. Lecturers worked in Databricks and co-founded dbt Learn so I decided to do this course first - and in the process, subtract from the ever growing number of Udemy courses that I haven't finished :p  
[^3]: This is the repo for the course: [complete-dbt-bootcamp-zero-to-hero](https://github.com/nordquant/complete-dbt-bootcamp-zero-to-hero)

Here's what I did today:
- Created a [Snowflake](https://www.snowflake.com/en/) trial account. It's good for 30 days with $400 credit.
- Setup Snowflake warehouse and permissions for `dbt`, ingested Airbnb data hosted in AWS S3 into the raw tables.
- In my local machine, installed Python 3.12.7 (I needed to downgrade from Python 3.13.0 as [it's not yet supported by dbt](https://docs.getdbt.com/faqs/Core/install-python-compatibility)). I also installed `virtualenv` using Homebrew but I realized I could have just installed it using `pip`. 
- Installed `dbt-snowflake` in the virtual environment (always use a Python virtual environment!)
- Created a dbt user directory `mkdir ~/.dbt`
- Initialized dbt project (`dbt init dbtlearn`) and connected it to the Snowflake account. I also learned just to delete the project directory when deleting a project (I made a configuration mistake! :p)
- Updated  `dbt_project.yml` to remove examples; deleted examples from the project directory
- Installed dbt plugin "Power User for dbt core" in VS Code and set it up.


### Github Copilot

As an aside, I got distracted because the lecturer's VS Code has Copilot enabled so I tried to setup mine. The free version is supposed to be one click and a Github authentication away but for some reason it's buggy in my IDE. Leaving it alone for now. 

## 0: 2024-11-16 (Sat) to 2025-01-08 (Wed) 
### DE Bootcamp
I finished Zach's Free YouTube Data Engineering bootcamp ([DataExport.io](https://www.dataexpert.io)) which started November last year and will run until February 7 (the deadline was extended from last day of January).

The topics covered were:
- Dimensional Data Modeling  
- Fact Data Modeling  
- Apache Spark Fundamentals  
- Applying Analytical Patterns  
- Real-time pipelines with Flink and Kafka  
- Data Visualization and Impact  
- Data Pipeline Maintenance  
- KPIs and Experimentation  
- Data Quality Patterns [^1]

Zach Wilson did a good job of explaining the topics (I'm also very impressed with how well he can explain the labs while writing code without missing a beat). The Data Expert community was also an incredible lot, as some of the setup and homeworks were complicated without prior exposure.

It was a challenging 6 weeks of my life with lectures, labs, and homeworks so much so that there was some lingering emptiness when my schedule freed up as I finished the bootcamp. I'm glad I went through it and it's a good jumping off point for my learning goal this year.

Sharing the link to [my certification](https://bootcamp.techcreator.io/certification/maryletteroa78207/free-bootcamp-completion).

[^1]: This is the repo for the bootcamp and many more: [data-engineering-handbook](https://github.com/DataExpert-io/data-engineer-handbook/)

**Footnotes**