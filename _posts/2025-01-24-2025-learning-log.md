---
layout: post
title: 2025 Learning Log
categories: [learning-log]
tags: [data-engineering]
mermaid: true
pin: true
---

In keeping up with my goal to do more learning this year, I'm allotting some time outside the daily grind for learning. 

My overarching goal is to explore popular data engineering tools such as `dbt`, and cloud technologies such as Azure Fabric and Snowflake. I haven't worked in Databricks since 2021 so it'll be a good opportunity to re-learn and catch up with the new developments.

Here are some notes and impressions of how the learning is coming along so far, written in reverse chronological order.

I've moved some notes to separate posts
- [dbt]({% link _posts/2025-02-02-dbt.md %})
- [ERDs and Mermaid üßú‚Äç‚ôÄÔ∏è]({% link _posts/2025-02-09-erd-and-mermaid.md %})
- [Python collections module]({% link _posts/2025-03-12-Python-collections-module.md %})
- [PyAutoGUI]({% link _posts/2025-03-13-PyAutoGUI.md %})
- [Snowflake]({% link _posts/2025-03-18-Snowflake.md %})
- [loguru]({% link _posts/2025-03-19-Python-logging-with-loguru.md %})
- [Apache Kafka]({% link _posts/2025-03-20-Apache-Kafka.md %})

## 48. 2025-04-22 (Tuesday) - Generative AI Solution Development

Topic: Assembling and Evaluating a RAG Application

This is the last section for the course, and so I've finished the course today üéâ

Steps
- Development
    - doc/ data pipeline development loop
    - chain development loop
- Expert/User testing
    - staging deployment to web chat app
- Offline evaluation
    - evaluation harness
- Production
    - production deployment
- Monitoring and logging
    - in all steps

MLflow, (including MLflow model registry) can facilitate development of RAG solutions

Evaluating RAG Pipeline - metrics
- retrieval related metrics
    - context precision
        - signal-to-noise ration for the retrieved context
    - context relevancy
        - measures relevancy of the retrieved context
    - context recall
        - measures the extent tp which all relevant entities and information are retrieved and mentioned in the context provided
- generation related metrics
    - faithfulness
        - measures the factual accuracy of the generated answer in relation to the provided context
    - answer relevancy
        - assesses how pertinent and applicable the generated response is to the user's initial query
    - answer correctness
        - measures the accuracy of the generated answer when compared to the ground truth



## 47. 2025-04-21 (Monday) - Generative AI Solution Development

Topic: Vector Search

Vector Database
- a database optimized to store and retrieve high-dimensional vectors such as embeddings; can handle sparse data i.e. data with lots of zeros
- uses include RAG, recommendation engines, similarity search

Measures of vector similarity
- distance metrics
- similarity metrics

Vector search strategies (at scale)
- K-nearest neighbors (KNN)
- Approximate nearest neighbors (ANN)
- Hierarchical navigable small worlds (HNSW)

Reranking
- a method of prioritizing documents most relevant to user's query

Mosaic AI vector search
- Databricks offering
- stores vector representation of data, plus metadata


## 46. 2025-04-15 (Tuesday) - Generative AI Solution Development

Topic: Preparing Data for RAG Solutions

Simple data prep process

- Ingest and pre-process external sources
- Chunk the texts
- Extract the embeddings from each chunk
- Save embeddings into vector store

Different ways to chunk data

- Context-aware chunking - chunk by sentence / paragraph / section
- Fixed-size chunking - divide by a specific number of tokens; simple and computationally cheap
- Experiment: Chunking by sentence, or chunking by multiple paragraphs
- Overlap chunks
- Windowed summarization - each chunk contains a windowed summary of previous few chunks; context-enriching
- Prior knowledge of user's query patterns can be helpful; ensure that the semantics on the query side are going to match the semantics on the index side 
- Summarization - sections of the documentations are summarized by LLMs and then embeddings are calculated from each summary

Tips on Embedding

- Choose the embedding model wisely. The embedding model should represent BOTH queries and documents
- Ensure similar embedding space for both queries and documents



## 45. 2025-04-14 (Monday) - Generative AI Solution Development

There is an on-going Databricks Event [Virtual Learning Festival](https://community.databricks.com/t5/events/virtual-learning-festival-9-april-30-april/ec-p/111620#M2286) that features learning pathways in data analytics, data engineering, machine learning, and generative AI.

I checked out the Generative AI Engineering pathway since, out of all the topics, this is the one I'm least familiar with (having used Databricks previously for DA/DE/ML).

I'm currently on the first course [Generative AI Development](https://customer-academy.databricks.com/learn/course/2706/generative-ai-solution-development)

Topic: From Prompt Engineering to RAG

Here are some interesting things I've learned:

Prompt Engineering Techniques
- Zero Shot / Few Shot programming
    - not using / using an example
- Prompt Chaining
    - tasks are broken into subtasks
    - the output of one prompt serves as the input for the next
- Chain-of-Thought prompting
    - guide the LLMs by prompting them to articulate their thought process step-by-step

Tips

- Prompts are model-specific; different models may require different prompts
- Format prompts; use delimiters, ask model to return structured output
- Guide the model for better responses
    - Ask the model not to hallucinate: "Do not make things up if you do not know. Say 'I do not have that information'."
    - Ask the model not to assume or probe sensitive information.
    - Ask the model not to rush to a solution

RAG 

- Retrieval augmented generation
- not to be confused with DAG (direct acyclic graph ) which, in data engineering, describes workflows or pipelines in orchestrations
- a pattern that can improve the efficacy of LLM applications by leveraging custom data
- done by retrieving data/documents relevant to a question or tasks and providing them as context to augment the prompts to an LLM so as to improve generation


## 44. 2025-04-11 (Friday) - Microsoft AI Skills Fest

Topics: Copilot AI Agent

I've viewed the topic [here](https://developer.microsoft.com/en-us/reactor/events/25295/), which is also a Microsoft Learn course [here](https://learn.microsoft.com/en-us/training/modules/github-copilot-agent-mode/). The repo for the activity / hands-on is also [here](https://github.com/continuous-copilot/build-applications-w-copilot-agent-mode).

My main takeaways are:
- Copilot Agent is really just a copilot. As a developer, you'd still need to know your stuff in order to steer the AI in the direction you want it. Case in point, the suggestions can make mistakes and/or the AI-generated code might not be ideal for production.
- It was fascinating to see how the agent can do self-healing: when an error is encountered, it's able to diagnose what the error means and provide the reason(s) and/or workaround(s) in order to solve the error.
- Still, this Copilot AI Agent seems pretty great for scaffolding code which we know can get pretty repetitive.

I've also noticed that my own VS Code Copilot has the "Agent" feature already which means this is already generally available.

## 43. 2025-04-10 (Thursday) - Microsoft AI Skills Fest

Topics: Writing effective prompts, Working with AI responsibly

Continuing on this video from Microsoft Skills Fest: [AI in Action: Unlock Productivity at work](https://www.youtube.com/watch?v=__vH5uFaZOE).

Also sharing my [AI Skills Fest Festival Participation Badge](https://learn.microsoft.com/en-us/users/maryletteroa-2351/achievements/pldanyn4).

### Writing effective prompts 

Best Practices:
- Be specific about what you want Copilot to do.
- Add some context to help Copilot understand what you're asking.
- Provide some examples for Copilot to use.
- Let Copilot know how you want the response to be formatted. 

The art of the prompt
- Goal: What do you want from Copilot
- Context: Why do you need it and who is involved?
- Source(s): What information or samples do you want Copilot to use?
- Expectations: How should Copilot respond to best fulfill your request?

### Working with AI responsibly

Checklist
- Use clear, specific prompts to get relevant AI output
- Verify accuracy by cross-checking AI-generated content with trusted sources
- Watch for biases (gender, racial, socioeconomic) and adjust content as needed
- Edit AI-generated content to ensure clarity, fairness, and alignment with your goals.
- Respect privacy - avoid sharing sensitive or personal data with AI tools
- Be transparent - disclose AI-generated content in professional or public use

Watch for common biases:
- Gender bias. AI systems can perpetuate gender stereotypes if trained on biases date.
- Racial bias. AI systems can exhibit racial bias, leasing to discriminatory outcomes.
- Socioeconomic bias. AI systems can favor individuals from higher socioeconomic backgrounds.
- Confirmation bias. AI models learn from existing patterns, so if the data is skewed, AI may reinforce one-sided perspectives.


## 42. 2025-04-09 (Wednesday) - VS Code, Microsoft AI Skills Fest

Topic: VS Code Step debugging

Add a breakpoint in the code. Click "Run and Debug"

`Step over` runs the next line. `Step in` runs the line inside loops or functions, `Step out` runs the next line outside the current loop or function.

Topic: AI in Action: Unlock Productivity at Work

I've registered for Microsoft's [AI Skills Fest](https://aiskillsfest.event.microsoft.com/) and viewed the session on [how AI can be used to be more productive at work](https://www.youtube.com/watch?v=__vH5uFaZOE). 

I learned about Copilot Agent that allows users to create basic AI agents without coding



## 41. 2025-04-03 (Thursday) - Apache Kafka

Topic: Controller, Producers, and Consumers

## 40. 2025-04-02 (Wednesday) - Apache Kafka

Topic: Spreading messages across partitions, partition Leader and Followers

## 39. 2025-03-31 (Monday) - Apache Kafka

Topics: Messages, Topics and partitions

## 38. 2025-03-27 (Thursday) - Apache Kafka

Topic: Kafka Topic

## 37. 2025-03-25 (Tuesday) - Apache Kafka

Topics: Apache Kafka, Broker, Zookeeper, Zookeeper ensemble multiple Kafka clusters, and default ports for Zookeeper, and broker

## 36. 2025-03-24 (Monday) - Apache Kafka

Topic: Producing and consuming messages

## 35. 2025-03-20 (Thursday) - Apache Kafka

I started learning [Apache Kafka]({% link _posts/2025-03-20-Apache-Kafka.md %}). I wanted to study Flink actually but since it comes downstream of Kafka, I figured I might as well learn a bit more about Kafka first. 

Topics: Introduction to Kafka, installation and starting the server using Docker compose, and from binary. 

## 34. 2025-03-19 (Wednesday) - Python 

Topic: [loguru]({% link _posts/2025-03-19-Python-logging-with-loguru.md %})

## 33. 2025-03-13 (Thursday) - Python 

Topic: [PyAutoGUI]({% link _posts/2025-03-13-PyAutoGUI.md %})

## 32. 2025-03-12 (Wednesday) - Python 

Topic: [Python collections module]({% link _posts/2025-03-12-Python-collections-module.md %})

## 31. 2025-03-11 (Tuesday) - Python `__main__`, refactoring if-else statement, slice, any, guard clause, function currying

This statement  `if __name__ == "__main__"` ensures that only the intended functions are run in the script, as importing a module also runs the module itself.

This if else statement

```python
if x > 2:
    print("b")
else:
    print("a")
```

Can be written like this

```python
print("b") if x > 2 else print("a")
```
I can't agree that the latter is readable though (or I'm just not used to it)

Using `any()` on an iterable

```python
numbers = [-1,-2-4,0,-3, -7]
has_positives = any(n > 0 for n in numbers)
```

Writing a Guard clause so that if a statement is not true, there's no need to run the rest of the code

`slice` object 

```python
numbers: list[int] = list(range(1, 11))
text: str = "Hello, world!"

rev: slice = slice(None, None, -1)
f_five: slice = slice(None, 5)

print(numbers[rev]) # [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]
print(text[rev]) # !dlrow, olleH
print(text[f_five]) # Hello
```

Currying creates specialized functions based on a general function

```python
def multiply_setup(a: float) -> Callable:
    def multiply(b: float) -> float:
        return a * b

    return multiply

double: Callable = multiply_setup(2)
triple: Callable = multiply_setup(3)

print(double(2)) # 4
print(triple(10)) # 30
```





Refs:
- [Quick Python Refactoring Tips](https://www.youtube.com/watch?v=rp1QR3eGI1k)
- [5 Good Python Habits](https://www.youtube.com/watch?v=I72uD8ED73U)
- [5 Uncommon Python Features](https://www.youtube.com/watch?v=sQ1Q96-Vhjk)


## 30. 2025-03-10 (Monday) - GraphQL

I was looking into resources for GraphQL and found these interesting bits

1. Microsoft Fabric now has GraphQL API data access layer
    - [GraphQL for SQL database in Microsoft Fabric](https://www.youtube.com/watch?v=nk1lU_aB0jI)
    - [What is Microsoft Fabric API for GraphQL?](https://learn.microsoft.com/en-us/fabric/data-engineering/api-graphql-overview)
2. The official GraphQL learning resource [GrapQL Learn](https://graphql.org/learn/)
3. A Python package to create GraphQL endpoints based on dataclasses [Stawberry](https://strawberry.rocks/)

Some terms:
- Schema - defines the structure of the data that can be returned
- Types - describes what data can be queried from the API
- Queries - used to retrieve data
- Mutations - used to modify data
- Subscriptions - used to retrieve real-time updates


## 29. 2025-03-06 (Thursday) - End of Snowflake course üéâ 


Topics: Third party tools, and best practices


## 28. 2025-03-05 (Wednesday) - Snowflake 

Topic: Snowflake Access Management


## 27. 2025-03-04 (Tuesday) - Snowflake

Topics: Materialized views, and data masking


## 26. 2025-03-03 (Monday) - Snowflake 

Topic: Snowflake streams


## 25. 2025-02-27 (Thursday) - Snowflake


Topics: Data sampling, and tasks


## 24. 2025-02-26 (Wednesday) - Snowflake

Topic: Data sharing


## 23. 2025-02-25 (Tuesday) - Snowflake


Topics: Table types, Zero-copy cloning, Swapping


## 22. 2025-02-24 (Monday) - Snowflake

Topics: Time travel, and fail safe


## 21. 2025-02-21 (Friday) - Snowflake 

Topic: Snowpipe


## 20. 2025-02-20 (Thursday) - Snowflake

Topics: Loading data from AWS, Azure, and GCP into Snowflake


## 19. 2025-02-19 (Wednesday) - Snowflake

Topics: Performance considerations, scaling up/down, scaling out, caching, and cluster keys


## 18. 2025-02-18 (Tuesday) - Snowflake

Topic: Loading unstructured data into Snowflake


## 17. 2025-02-17 (Monday) - Snowflake 

Topics: Copy options, rejected records, load history


## 16. 2025-02-13 (Thursday) - Snowflake 

Topics: COPY command, transformation, file format object


## 15. 2025-02-12 (Wednesday) - Snowflake 

Topics: Editions, pricing and cost monitoring, roles



## 14. 2025-02-11 (Tuesday) - Snowflake 

Taking a step towards my goals this year, I've started doing a deep-dive into Snowflake through learning about it in this course [Snowflake Masterclass](https://www.udemy.com/course/snowflake-masterclass/).

Topics: Setup, architecture overview, loading data


## 13. 2025-02-10 (Monday) - streamlit

I've checked out [Streamlit](https://streamlit.io), a Python library for creating  web apps. Unlike other libraries or frameworks like Django or even Flask, Streamlit is able to spin up a web app fast using simple syntax. It is specially useful for data science and machine learning projects. 

It is designed for quickly creating a data-driven web application. I'm not clear if it's "production-quality" and opinions seem to be divided and depend on requirements or use-case.

e.g.

```python
import streamlit as st
import numpy as np
import pandas as pd

st.title('A Sample Streamlit App')

st.markdown('## This is a line chart')

chart_data = pd.DataFrame(
     np.random.randn(20, 3),
     columns=['a', 'b', 'c'])

st.line_chart(chart_data)

st.markdown('## This is a table')

st.dataframe(chart_data)

```

<center><img src="/assets/images/learning-log/streamlit-charts.png" alt="streamlit-charts.png" width="500"/></center>

More features in the [Docs](https://docs.streamlit.io/)


## 12. 2025-02-09 (Sunday) - ERD, Mermaid

Today, I reviewed [ERDs and revisited Mermaid üßú‚Äç‚ôÄÔ∏è]({% link _posts/2025-02-09-erd-and-mermaid.md %})


## 11. 2025-02-06 (Thursday) - docker, dbt-duckdb, Duckdb resources

### docker

I wanted to check Docker üê≥ and see if I can try and create a container for a data pipeline. Well, that is the goal but since I don't use Docker these days, I needed to reacquaint myself with it first.

These are some of the intro I found
- <i class="fa-brands fa-youtube"></i> [The intro to Docker I wish I had when I started ](https://www.youtube.com/watch?v=Ud7Npgi6x8E&t=563s)
- <i class="fa-brands fa-youtube"></i> [ Learn Docker in 7 Easy Steps - Full Beginner's Tutorial](https://www.youtube.com/watch?v=gAkwW2tuIqE)
- <i class="fa-brands fa-youtube"></i> [Containerize Python Applications with Docker](https://www.youtube.com/watch?v=0TFWtfFY87U&t=559s)

an example of `Dockerfile`
```bash
FROM python:3.9
ADD main.py .
RUN pip install scikit-learn
CMD ["python", "./main.py"]
```

`.dockerignore` specifies the files or paths that are excluded when copying to the container

Ideally, there is only one process per container

`docker-compose.yml` - for running multiple containers at the same time

```bash
version: '3'
services:
    web:
        build: .
        ports:
            - "8080:8080"
    db:
        image: "mysql"
        environment:
            MYSQL_ROOT_PASSWORD: password
        volumes:
            - db-data:/foo

volumes:
    db-data:
```
`docker-compose up` to run all the containers together  
`docker-compose down` to shutdown all containers

Will revisit this topic in the succeeding days. 

### dbt-duckdb

Here is the repo for the adapter: [dbt-duckdb](https://github.com/duckdb/dbt-duckdb)

Installation should be
- `pip3 install dbt-duckdb`

### Duckdb resources

Putting these resouces here:
- [Duckdb tutorial for beginners](https://motherduck.com/blog/duckdb-tutorial-for-beginners/)
- <i class="fa-brands fa-youtube"></i> YT Channel [@motherduckdb](https://www.youtube.com/@motherduckdb)

I just realized that this feature solves some of my challenging tasks: Since Duckdb is able to read a CSV and execute SQL query on it, there's no need to open a raw CSV just to check the sum of columns for example. The computation is in-memory too by default so no need to persist a database for quick analysis. 

```sql
SELECT * FROM read_csv_auto('path/to/your/file.csv');
```

or just using the terminal
```bash
$ duckdb -c "SELECT * FROM read_parquet('path/to/your/file.parquet');"
```


## 10. 2025-02-05 (Wednesday) - dbt-Fabric, Fireducks

### dbt-Fabric
I was looking around for how to integrate dbt to Azure, and I found these resouces
- [Setup dbt for Fabric Data Warehouse](https://learn.microsoft.com/en-us/fabric/data-warehouse/tutorial-setup-dbt)
    - <i class="fa-brands fa-youtube"></i> [How to run dbt in Microsoft Fabric](https://www.youtube.com/watch?v=Zmh3Z8nhoDw&t=639s)
- Also [dbt cloud is available for Azure Fabric](https://www.getdbt.com/blog/dbt-cloud-is-now-available-for-microsoft-fabric)
    - <i class="fa-brands fa-youtube"></i> [Configure and Create DBT Model for Fabric Warehouse using dbt Cloud ](https://www.youtube.com/watch?v=7UX5qy45c4g)


The process looks straight-forward
- Install the adapter in the virtual environment with Python 3.7 and up `pip install dbt-fabric`
- Make sure to have the [Microsoft ODBC Driver for Sql Server](https://learn.microsoft.com/en-us/sql/connect/odbc/download-odbc-driver-for-sql-server?view=sql-server-ver16#download-for-windows) installed
- Add an existing Fabric warehouse
- The dbt profile in the home directory needs to be setup
- Connect to the Azure warehouse, do the authentication
- Check the connections
- Then the dbt project can be built

Aside from Fabric, dbt also has integrations with other Azure data platforms:
- [Synapse](https://docs.getdbt.com/docs/core/connect-data-platform/azuresynapse-setup)
- [Data Factory](https://learn.microsoft.com/en-us/fabric/data-factory/apache-airflow-jobs-dbt-fabric)
- [SQL Server](https://docs.getdbt.com/docs/core/connect-data-platform/mssql-setup) (SQL Server 2017, SQL Server 2019, SQL Server 2022 and Azure SQL Database)

Here is the docs for other dbt core platform connections
- [dbt Core data platform connections ](https://docs.getdbt.com/docs/core/connect-data-platform/about-core-connections)

### Fireducks
There's another duck in the data space: [Fireducks](https://fireducks-dev.github.io/)
. It's not related to DuckDB, instead it allows existing code using `pandas` to be more performant; it's fully compatible with Pandas API. 

This means there's [zero learning cost](https://medium.com/@fireducks/introduction-to-fireducks-get-performance-beyond-pandas-with-zero-learning-cost-8694d4eab9c6) as all that's needed is to replace `pandas` with `fireducks` and the code should be good to go

```python
# import pandas as pd
import fireducks.pandas as pd
```

or via terminal (no need to change the import)

```bash
python3 -m fireducks.pandas main.py
```

Whereas for `polars`, the code would need to be rewritten; polars code is closer to PySpark.

e.g.
```python
import pandas as pd
import polars as pl
from pyspark.sql.functions import col

# load the file ...
#
# Filter rows where 'column1' is greater than 10
# pandas
filtered_df = df[df['column1'] > 10]
# polars
filtered_df = df.filter(pl.col('column1') > 10
# PySpark
filtered_df = df.filter(col('column1') > 10)
```

Here's a comparison of the performance: [Pandas vs. FireDucks Performance Comparison](https://blog.dailydoseofds.com/p/pandas-vs-fireducks-performance-comparison). It surpassed both `polars` and `pandas` 

## 9: 2025-02-04 (Tuesday) - Snowflake, Duckdb, Isaac Asimov Books

I'm taking a break from learning dbt, and switched focus to learning about databases: Snowflake, and Duckdb. Also, I took a bit of a break so that I can catch up on reading Isaac Asimov's series.

### Snowflake virtual warehouse

I found this free introducton in Udemy [Snowflake Datawarehouse & Cloud Analytics - Introduction](https://www.udemy.com/course/introduction-to-snowflake-cloud-data-warehouse-analytics/). This may not be the best resource out there though as it hasn't been updated. 
However, I learned about provisioning a virtual warehouse in Snowflake with the following example commands


Virtual warehouse
- collection of compute resources (CPUs and allocated memory)
- needed to query data from snowflake, and load data into snowflake
- autoscaling is available in enterprise version but not in standard version

Create a virtual warehouse
```sql
CREATE WAREHOUSE TRAINING_WH
WITH
	WAREHOUSE_SIZE = XSMALL --size
	AUTO_SUSPEND = 60 -- if idle for 60s will be suspended automatically
	AUTO_RESUME = TRUE -- if use executes a query, it will automatically resume on it's own without having to manually restarted the warehouse
	INITIALLY_SUSPEND = TRUE
	STATEMENT_QUEUED_TIMEOUT_IN_SECONDS = 300
	STATEMENT_TIMEOUT_IN_SECONDS = 600;
```

Alternatively, use the user interface as well

Create database

```sql
CREATE DATABASE SALES_DB
DATA_RETENTION_TIME_IN_DAYS = 0
COMMENT = 'Ecommerce sales info';
```

Create schema
```sql
create schema Sales_Data;
create schema Sales_Views;
create schema Sales_Stage;
```

Use the warehouse
```sql
USE WAREHOUSE_TRAINING_WH;
USE DATABASE SALES_DB;
USE SCHEMA Sales_Data;
```

Command to find the current environment
```sql
SELECT CURRENT_DATABASE(), CURRENT_SCHEMA(), CURRENT_WAREHOUSE();
```

Here's a [demo of Snowflake features](https://www.youtube.com/watch?v=9PBvVeCQi0w). I learned that Snowflake has it's own data marketplace, and also has a dashboard feature.

Here are others resources for Snowflake
-  YT channel [@snowflakedevelopers](https://www.youtube.com/@snowflakedevelopers) 
- [Developers website](https://www.snowflake.com/en/developers/)
- [Snowflake documentation](https://docs.snowflake.com/en/index)
- [Snowflake community](https://community.snowflake.com/s/)

### Duckdb overview

Another database that I've been hearing about is [Duckdb](https://duckdb.org/), and I'm honestly very interested in this one as it is light-weight and open-source. It can run in a laptop, and can process GBs of data fast.

It's a file-based database which reminds me of SQLite except that DuckDB is mostly for analytical purposes (OLAP) rather than transactional (OLTP). It utilizes vectorized execution as opposed to tuple-at a time (row-based database), or column-at-a-time execution (column-based database). It sits somewhere between row-based and column-based in that the data is processed by columns but operates on batches of data at a time. Because of this, it is more memory efficient than column execution (e.g. pandas).


Here is [Gabor Szarnyas' presentation about Duckdb](https://www.youtube.com/watch?v=9Rdwh0rNaf0) which talks in detail about Duckdb capabilities. 


Duckdb isn't a one-to-one comparison with Snowflake, though as it can only scale by memory, and is not distributed (nor with Apache Spark for that matter). It also runs locally. A counterpart to this is [MotherDuck](https://motherduck.com/), which is a cloud data warehousing solution built on top of Duckdb (kind of like [dbt Cloud](https://www.getdbt.com/product/dbt-cloud) to dbt core).

As a side note, I was delighted to learn that DuckDB has a SQL command to exclude columns! (Lol I know but you have no idea how cumbersome it is to write all 20+ columns only to exclude a few :p)

example from [Duckdb Snippets](https://duckdbsnippets.com/snippets/2/using-the-exclude-function-in-duckdb)
```sql
// This will select all information about ducks
// except their height and weight
SELECT * EXCLUDE (height, weight) FROM ducks;
```
Whereas the top [Stack Overflow solution](https://stackoverflow.com/questions/729197/exclude-a-column-using-select-except-columna-from-tablea) for this is 
```sql
/* Get the data into a temp table */
SELECT * INTO #TempTable
FROM YourTable
/* Drop the columns that are not needed */
ALTER TABLE #TempTable
DROP COLUMN ColumnToDrop
/* Get results and drop temp table */
SELECT * FROM #TempTable
DROP TABLE #TempTable
```

I know explicitly writing column names is for "contracts" and exclude isn't very production quality but it would be immensely useful in CTEs where the source columns have already been defined previously.

Okay end of side note :p 

Do you think it's normal to fan over a database? No pun intended. : ))

### Three Laws of Robotics

I'm currently reading Isaac Asimov books. I'm on Book 2 (Foundation and Empire) of the [Foundation Series](https://en.wikipedia.org/wiki/Foundation_(book_series)). I kind of wanted to read the books before I watch Apple TV's Foundation but I realized the series is *totally different* from the books. It was like preparing for an exam only to get entirely out-of-scope questions (which has happened too many times before :p)

Spoiler: <details>Book 1 is logical and clever strategies towards starting a foundation planet, whereas the Apple series is all *pew*, *pew*, guns etc. Not Book 2 though, it's all *pew*, *pew*, space battle :p </details>

Anyway, the [Three Laws of Robotics](https://en.wikipedia.org/wiki/Three_Laws_of_Robotics) (aka Asimov's Laws) didn't originate in the Foundation Books (*I just really want to talk about it* :p ) but in one of his short stories in the [I, Robot](https://en.wikipedia.org/wiki/I,_Robot) collection. 
    
Spoiler: <details>But isn't Demerzel from Apple TV's series inspired by this? Was it a cross-over? But didn't the Robot series and the Foundation Series had cross-overs also?.</details>

The Three Laws of Robotics state:
1. A robot may not injure a human being or, through inaction, allow a human being to come to harm.
2. A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.
3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.

The Three Laws of Robotics originally came from a science-fiction story in the 1940's but it's amazing how forward-looking it is. I'm not really into science-fiction books (I'd rather not mix the two :p) but I'm pulled into Asimov's world nonetheless.



## 8: 2025-02-02 (Sun) - dbt

### dbt - certifications and finished the course
I've finished the course today üéâ. 

I've also moved the dbt notes into [another post]({% link _posts/2025-02-02-dbt.md %}) to keep the log tidier.

The last section is an interview about the official dbt certifications. I'm still not sure about doing the certification at this point (I kind of want to get more hands-on time with the tool first) but I like what the interviewee said about doing certifications - you learn a lot more about the tool, faster compared to just using it everyday. For me, I do get a lot of gains studying for certs. If I didn't have those stock knowledge in the first place, it would have been a lot harder to think of other, better ways of approaching a problem. Mostly my qualms about doing certs is how expensive they are! :p


## 7: 2025-02-01 (Sat) - dbt

### dbt - Advance Power user dbt core, introducing dbt to the company

Today isn't as technical as the last couple of days.  I've covered more features of the Power User for dbt core extension powered with AI, and tips on introducing dbt to the company.

The course is wrapping up as well, and I only have one full section left about certifications. 

I learned about 
- Advance Power User dbt core
- Introducing dbt to the company

## 6: 2025-01-31 (Fri) - dbt

### dbt - variables, and dagster

Today I've covered dbt variables and orchestration with dagster. It was my first time setting up dagster. I actually liked dagster because the integration with dbt is tight. I was a bit overwhelmed though with all the coding at the backend to setup the orchestration. It might get easier if I take a deeper look at it. For now, it seems like a good tool to use with dbt. 

## 5: 2025-01-30 (Thu) - dbt

### dbt - great expectations, debugging, and logging

I've covered these topics today: `dbt-great-expectations`, debugging, and logging. 

The dbt-great-expectations package, though not really a port of the Python package, contains many tests that are useful for checking the model. I'm glad `ge` was adapted into dbt as it's also one of the popular data testing tool in Python.

## 4: 2025-01-29 (Wed)  - dbt

### dbt - snapshots, tests, macros, packages, docs, analyses, hooks, exposures.

Today was pretty full. I've covered dbt snapshots, tests, macros, third-party packages, documentation, analyses, hooks, and exposures. Not sure how I completed all of these today but these are pretty important components of dbt. I'm amazed about the documentation support in this tool, and snapshot is another feature in which I say "where has this been all my life?" To think that SCD is that straight-forward in dbt.

## 3: 2025-01-26 (Sun) - dbt

### dbt - seeds, and source

Today, I've covered dbt seeds, and source. At first I thought - why did they need to relabel these CSV files and raw tables? The terminologies were a bit confusing but I guess I should just get used to these.

## 2: 2025-01-25 (Sat) - dbt, books, Python package, Copilot

### dbt - models, and materialization

I've covered dbt models, and materialization. These are pretty core topics in dbt - because dbt is all about models! 

### Books for LLM, and Polars

Here are some materials I came upon today

LLM Engineer's Handbook [[Amazon]](https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/) [[Github]](https://github.com/PacktPublishing/LLM-Engineers-Handbook)
- a resource when I get around to exploring LLMs 

Polars Cookbook [[Amazon]](https://www.amazon.com/Polars-Cookbook-practical-transform-manipulate/dp/1805121154/)
- [Polars](https://pola.rs/) is a fast alternative to [Pandas](https://pandas.pydata.org/docs/) specially for large datasets

### pandera

> `pandera` is a Union.ai open source project that provides a flexible and expressive API for performing data validation on dataframe-like objects to make data processing pipelines more readable and robust. Dataframes contain information that pandera explicitly validates at runtime. - [[Docs]](https://pandera.readthedocs.io/en/stable/)

Here's an example from the docs. It's interesting because it seems like a lighter version of Great Expectations wherein the data can be further validated using ranges and other conditions. It's powerful for dataframe validations.

```python
import pandas as pd
import pandera as pa

# data to validate
df = pd.DataFrame({
    "column1": [1, 4, 0, 10, 9],
    "column2": [-1.3, -1.4, -2.9, -10.1, -20.4],
    "column3": ["value_1", "value_2", "value_3", "value_2", "value_1"],
})

# define schema
schema = pa.DataFrameSchema({
    "column1": pa.Column(int, checks=pa.Check.le(10)),
    "column2": pa.Column(float, checks=pa.Check.lt(-1.2)),
    "column3": pa.Column(str, checks=[
        pa.Check.str_startswith("value_"),
        # define custom checks as functions that take a series as input and
        # outputs a boolean or boolean Series
        pa.Check(lambda s: s.str.split("_", expand=True).shape[1] == 2)
    ]),
})

validated_df = schema(df)
print(validated_df)
```

In [Pydantic](https://docs.pydantic.dev/latest/), something like this can also be used

```python
from pydantic import BaseModel, conint, confloat

class Product(BaseModel):
    quantity: conint(ge=1, le=100)  # Validates that quantity is between 1 and 100
    price: confloat(gt=0, le=1000)   # Validates that price is greater than 0 and less than or equal to 1000

product = Product(quantity=10, price=500)
print(product)

```

### Copilot installation, an update

The VS Code plugin that I was trying to install yesterday is working now and I am able to access chat on the side panel as well as see the prompts on screen. Not really sure what fixed it but it could be the reboot of my computer.

## 1: 2025-01-24 (Fri) - dbt, Copilot

### dbt - introduction ,and setting up

I'm going through the dbt course in Udemy [The Complete dbt (Data Build Tool) Bootcamp: Zero to Hero](https://www.udemy.com/course/complete-dbt-data-build-tool-bootcamp-zero-to-hero-learn-dbt/). I've setup a dbt project and created a Snowflake account.

It was between this and [dbt Learn](https://www.getdbt.com/dbt-learn) platform - I might go back to that for review later. Lecturers worked in Databricks and co-founded dbt Learn so I decided to do this course first - and in the process, subtract from the ever growing number of Udemy courses that I haven't finished :p  

### Github Copilot

As an aside, I got distracted because the lecturer's VS Code has Copilot enabled so I tried to setup mine. The free version is supposed to be one click and a Github authentication away but for some reason it's buggy in my IDE. Leaving it alone for now. 

## 0: 2024-11-16 (Sat) to 2025-01-08 (Wed) - DataExpert Data Engineering Bootcamp
### DE Bootcamp
I finished Zach's Free YouTube Data Engineering bootcamp ([DataExport.io](https://www.dataexpert.io)) which started November last year and will run until February 7 (the deadline was extended from last day of January).

The topics covered were:
- Dimensional Data Modeling  
- Fact Data Modeling  
- Apache Spark Fundamentals  
- Applying Analytical Patterns  
- Real-time pipelines with Flink and Kafka  
- Data Visualization and Impact  
- Data Pipeline Maintenance  
- KPIs and Experimentation  
- Data Quality Patterns

Zach Wilson did a good job of explaining the topics (I'm also very impressed with how well he can explain the labs while writing code without missing a beat). The Data Expert community was also an incredible lot, as some of the setup and homeworks were complicated without prior exposure.

It was a challenging 6 weeks of my life with lectures, labs, and homeworks so much so that there was some lingering emptiness when my schedule freed up as I finished the bootcamp. I'm glad I went through it and it's a good jumping off point for my learning goal this year.

Sharing the link to [my certification](https://bootcamp.techcreator.io/certification/maryletteroa78207/free-bootcamp-completion).
