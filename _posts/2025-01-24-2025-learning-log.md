---
layout: post
title: 2025 Learning Log
categories: [learning-log]
tags: [data-engineering]
---

In keeping up with my goal to do more learning this year, I'm allotting some time outside the daily grind for learning. 

My overarching goal is to explore popular data engineering tools such as `dbt`, and cloud technologies such as Azure Fabric and Snowflake. I haven't worked in Databricks since 2021 so it'll be a good opportunity to re-learn and catch up with the new developments.

Here are some notes and impressions of how the learning is coming along so far, written in reverse chronological order.

## 3: 2025-01-26 (Sun)

### dbt Seeds and Sources

#### Seeds
- local files yet to be uploaded to the warehouse using dbt
- contained in this path `dbtlearn/seeds` e.g. `dbtlearn/seeds/seed_full_moon_dates.csv`
- in the project YAML file, the paths of seeds are defined as

```yaml
seed-paths: ["seeds"]
```
- `dbt seed` command populates seed in Snowflake; this results in a table. (Snowflake can infer the column schema from file)

#### Source
- an abstraction layer that sits on the top of the input (raw) tables e.g. those defined in models `/dbtlearn/models/src/src_*.sql`
- contains extra features such as checking for data freshness
- to define models as source, create a separate `source.yml` file under the src model directory `dbtlearn/models/sources.yml`

e.g.
```yaml
version: 2

sources:
    - name: airbnb
      schema: raw
      tables:
        - name: listings
          identifier: raw_listings

        - name: hosts
          identifier: raw_hosts

        - name: reviews
          identifier: raw_reviews
          loaded_at_field: date
          freshness:
            warn_after: {count: 1, period: hour}
            error_after: {count: 24, period: hour}
```

- in `models/src/src_listings.sql` instead of directly referencing the Snowflake table as `AIRBNB.RAW.RAW_LISTINGS`, it can now be abstracted to
    - `{{ source('airbnb', 'listings') }}` using names listed in the source YAML file

e.g. 
```sql
with raw_listings as (
select * from {{ source('airbnb', 'listings') }}
)

-- continue code here
```

- `dbt compile` checks template tags, and connections; dbt goes through models, YAML files, tests etc and checks if all references are correct

#### Source freshness

- in the source YAML file, these lines under `reviews` define freshness
```yaml
          loaded_at_field: date
          freshness:
            warn_after: {count: 1, period: hour}
            error_after: {count: 24, period: hour}
```
- this means
    - give warning if there's no data that's less than an hour fresh
    - return an error if there are no new data in 24 hours

- `dbt source freshness` is the command to use to  check the freshness


### YAML extension recognition in VS Code

My IDE recognizes `*.yaml` extensions as YAML but not `*.yml`. To fix this:
- `CMD + Shift + P`, type "Change language mode" and choose `YAML` language

## 2: 2025-01-25 (Sat)

### dbt models and materialization

Today,  I worked on writing dbt models and materializing them into Snowflake.

<center><img src="/assets/images/learning-log/dbt-project-tables.png" alt="tables-graph" width="500"/></center>

Here's the final outcome in the data warehouse

<center><img src="/assets/images/learning-log/tables-in-wh.png" alt="tables-in-wh" width="200"/></center>

The project YAML file has been modified to follow these defaults; the `+` sign before `+materialized` means that it's a dbt keyword and not a directory
```yml
models:
  dbtlearn:
    +materialized: view
    dim:
      +materialized: table
    src:
      +materialized: ephemeral
```

The following configuration was added for the the two dim tables so they materialized as views instead of tables, overriding the default state in the project YAML.

```sql
{​{
    config(
        materialized = 'view'
    )
}​}
```

Using the dbt plugin was also convenient in that when testing the SQL, I only needed to run it within the editor and it's able to return the results of the query (no need to switch into Snowflake's interface). It can also be used to build the model using GUI.

<center><img src="/assets/images/learning-log/dbt-project-using-plugin.png" alt="tables-graph" width="500"/></center>

`dbt` models are SQL definitions that can be materialized  (or not) into tables or views. These are stored as SQL files under `dbtlearn/models/`. They consist of `select` statements but contain other features such as: semantic dependencies with other models, scripts and macros. 

`dbtprpoject/target/run/dbtlearn/models` - contains the final compiled models. Files here can be investigated when troubleshooting.

CTE (common table expressions) are widely used and recommended as they are readable, easy to maintain, and make complex queries readable. They are similar to views but are not stored

Commands to build models include:
- `dbt run` - materializes the models
- `dbt run --full-refresh` - rebuilds all tables including incremental tables


There are four (4) types of materialization in dbt

type | use | don't use
-| - | - 
view | light weight representation;<br>the model isn't being reused often | the model is being read several times
table | the model is being read repeatedly | single-use models; incremental load
incremental | fact, event tables;<br>appends to tables | update historical records
ephemeral | i.e. no materialization;<br>want to use an alias to a column | model is read several times 


Here's an example of a dbt model using an incremental load, and [Jinja](https://jinja.palletsprojects.com/en/stable/)
```sql
{​{
    config(
        materialized = 'incremental',
        on_schema_change = 'fail'
    )
}​}
with src_reviews as (
    select * from {{ ref('src_reviews') }}
)

select * from src_reviews
where review_text is not null

{​% if is_incremental() %​}
    and review_date > (select max(review_date) from {​{this}​})
{​% endif %​}
```

When switching from a materialized model to an ephemeral model, delete the tables manually in the data warehouse as dbt won't delete the tables or views itself.

Here are other things I came across today that are interesting:

### Books for LLM, and Polars
LLM Engineer's Handbook [[Amazon]](https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/) [[Github]](https://github.com/PacktPublishing/LLM-Engineers-Handbook)
- a resource when I get around to exploring LLMs 

Polars Cookbook [[Amazon]](https://www.amazon.com/Polars-Cookbook-practical-transform-manipulate/dp/1805121154/)
- [Polars](https://pola.rs/) is a fast alternative to [Pandas](https://pandas.pydata.org/docs/) specially for large datasets

### pandera

> `pandera` is a Union.ai open source project that provides a flexible and expressive API for performing data validation on dataframe-like objects to make data processing pipelines more readable and robust. Dataframes contain information that pandera explicitly validates at runtime. - [[Docs]](https://pandera.readthedocs.io/en/stable/)

Here's an example from the docs. It's interesting because it seems like a lighter version of Great Expectations wherein the data can be further validated using ranges and other conditions. It's powerful for dataframe validations.

```python
import pandas as pd
import pandera as pa

# data to validate
df = pd.DataFrame({
    "column1": [1, 4, 0, 10, 9],
    "column2": [-1.3, -1.4, -2.9, -10.1, -20.4],
    "column3": ["value_1", "value_2", "value_3", "value_2", "value_1"],
})

# define schema
schema = pa.DataFrameSchema({
    "column1": pa.Column(int, checks=pa.Check.le(10)),
    "column2": pa.Column(float, checks=pa.Check.lt(-1.2)),
    "column3": pa.Column(str, checks=[
        pa.Check.str_startswith("value_"),
        # define custom checks as functions that take a series as input and
        # outputs a boolean or boolean Series
        pa.Check(lambda s: s.str.split("_", expand=True).shape[1] == 2)
    ]),
})

validated_df = schema(df)
print(validated_df)
```

In [Pydantic](https://docs.pydantic.dev/latest/), something like this can also be used

```python
from pydantic import BaseModel, conint, confloat

class Product(BaseModel):
    quantity: conint(ge=1, le=100)  # Validates that quantity is between 1 and 100
    price: confloat(gt=0, le=1000)   # Validates that price is greater than 0 and less than or equal to 1000

product = Product(quantity=10, price=500)
print(product)

```

### Copilot installation, an update
The VS Code plugin that I was trying to install yesterday is working now and I am able to access chat on the side panel as well as see the prompts on screen. Not really sure what fixed it but it could be the reboot of my computer.

## 1: 2025-01-24 (Fri)
### dbt and Snowflake setup
I'm going through the dbt course in Udemy [The Complete dbt (Data Build Tool) Bootcamp: Zero to Hero](https://www.udemy.com/course/complete-dbt-data-build-tool-bootcamp-zero-to-hero-learn-dbt/).[^2] [^3]


[^2]: It was between this and [dbt Learn](https://www.getdbt.com/dbt-learn) platform - I might go back to that for review later. Lecturers worked in Databricks and co-founded dbt Learn so I decided to do this course first - and in the process, subtract from the ever growing number of Udemy courses that I haven't finished :p  
[^3]: This is the repo for the course: [complete-dbt-bootcamp-zero-to-hero](https://github.com/nordquant/complete-dbt-bootcamp-zero-to-hero)

Here's what I did today:
- Created a [Snowflake](https://www.snowflake.com/en/) trial account. It's good for 30 days with $400 credit.
- Setup Snowflake warehouse and permissions for `dbt`, ingested Airbnb data hosted in AWS S3 into the raw tables.
- In my local machine, installed Python 3.12.7 (I needed to downgrade from Python 3.13.0 as [it's not yet supported by dbt](https://docs.getdbt.com/faqs/Core/install-python-compatibility)). I also installed `virtualenv` using Homebrew but I realized I could have just installed it using `pip`. 
- Installed `dbt-snowflake` in the virtual environment (always use a Python virtual environment!)
- Created a dbt user directory `mkdir ~/.dbt`
- Initialized dbt project (`dbt init dbtlearn`) and connected it to the Snowflake account. I also learned just to delete the project directory when deleting a project (I made a configuration mistake! :p)
- Updated  `dbt_project.yml` to remove examples; deleted examples from the project directory
- Installed dbt plugin "Power User for dbt core" in VS Code and set it up.

### Github Copilot
As an aside, I got distracted because the lecturer's VS Code has Copilot enabled so I tried to setup mine. The free version is supposed to be one click and a Github authentication away but for some reason it's buggy in my IDE. Leaving it alone for now. 

## 0: 2024-11-16 (Sat) to 2025-01-08 (Wed) 
### DE Bootcamp
I finished Zach's Free YouTube Data Engineering bootcamp ([DataExport.io](https://www.dataexpert.io)) which started November last year and will run until February 7 (the deadline was extended from last day of January).

The topics covered were:
- Dimensional Data Modeling  
- Fact Data Modeling  
- Apache Spark Fundamentals  
- Applying Analytical Patterns  
- Real-time pipelines with Flink and Kafka  
- Data Visualization and Impact  
- Data Pipeline Maintenance  
- KPIs and Experimentation  
- Data Quality Patterns [^1]

Zach Wilson did a good job of explaining the topics (I'm also very impressed with how well he can explain the labs while writing code without missing a beat). The Data Expert community was also an incredible lot, as some of the setup and homeworks were complicated without prior exposure.

It was a challenging 6 weeks of my life with lectures, labs, and homeworks so much so that there was some lingering emptiness when my schedule freed up as I finished the bootcamp. I'm glad I went through it and it's a good jumping off point for my learning goal this year.

Sharing the link to [my certification](https://bootcamp.techcreator.io/certification/maryletteroa78207/free-bootcamp-completion).

[^1]: This is the repo for the bootcamp and many more: [data-engineering-handbook](https://github.com/DataExpert-io/data-engineer-handbook/)
