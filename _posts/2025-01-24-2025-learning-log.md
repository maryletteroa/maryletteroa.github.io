---
layout: post
title: 2025 Learning Log
categories: [learning-log]
tags: [data-engineering]
---

In keeping up with my goal to do more learning this year, I'm allotting some time outside the daily grind for learning. 

My overarching goal is to explore popular data engineering tools such as `dbt`, and cloud technologies such as Azure Fabric and Snowflake. I haven't worked in Databricks since 2021 so it'll be a good opportunity to re-learn and catch up with the new developments.

Here are some notes and impressions of how the learning is coming along so far, written in reverse chronological order.

## 6: 2025-01-31 (Fri)

Today I've covered dbt variables and orchestration with dagster.

### Variables
There are two kinds
- Jinja variables
	- define and use them in Jinja
- dbt variables
	- dbt specific-variables which can be passed to dbt through the command line or the project YAML

#### Jinja variables
- can be defined using `set`
- e.g. create a macro in `dbtlearn/macros/variables.sql`

```sql
{​% macro learn_variables() %​}

    {​% set your_name_jinja = "FooBar" %​}
    {​{ log("Hello " ~ your_name_jinja, info=True) }​}

{​% endmacro %​}
```

#### dbt variables
- aka [Project variables](https://docs.getdbt.com/docs/build/project-variables)
- variables that dbt manages for you
- pass their values from the command line or project YAML

```sql
{​% macro learn_variables() %​}

    {​{ log("Hello dbt user " ~ var("user_name") ~ "!", info=True)}​}

{​% endmacro %​}
```

- in commandline, specify the username `dbt run-operation learn_variables --vars "{user_name: foobar}"`

### Setting default values
- can be set in `var()` but would need to set default value whenever you set the variable

```sql
{​% macro learn_variables() %​}
    {​{ log("Hello dbt user " ~ var("user_name", "NO USERNAME IS SET") ~ "!", info=True)}​}

{​% endmacro %​}
```
- another way is to define the default value in project YAML file
	- will overwrite default in macro if no `user_name` is passed in the command line

```yaml
vars:
  user_name: default_user_name_for_this_project
```

#### Using data ranges to make incremental models production-ready
- standard practice to use parameterization for data ranges to backfill
- check variable existence `{​% if var("start_date", False) and var("end_date", False) %​}`
- update `dbtlearn/models/fct/fct_reviews.sql` below

```sql
{​{
    config(
        materialized = 'incremental',
        on_schema_change = 'fail'
    )
}​}
with src_reviews as (
    select * from {​{ ref('src_reviews') }​}
)

select 
{​{ dbt_utils.generate_surrogate_key(['listing_id', 'review_date', 'reviewer_name', 'review_text'])}​} as review_id,
* from src_reviews
WHERE review_text is not null
{​% if is_incremental() %​}
  {​% if var("start_date", False) and var("end_date", False) %​}
    {​{ log('Loading ' ~ this ~ ' incrementally (start_date: ' ~ var("start_date") ~ ', end_date: ' ~ var("end_date") ~ ')', info=True) }​}
    AND review_date >= '{​{ var("start_date") }​}'
    AND review_date < '{​{ var("end_date") }​}'
  {​% else %​}
    AND review_date > (select max(review_date) from {​{ this }​})
    {​{ log('Loading ' ~ this ~ ' incrementally (all missing dates)', info=True)}​}
  {​% endif %​}
{​% endif %​}
```

- if this is not an incremental load (full refresh), or if it is but has no start date and end date parameters, proceed as the naive implementation i.e.load review date src to fct model if review is not null and review date is after the max date
- if this is an incremental load but has a start date and end date, then include extra conditions of having the review date in between start date and end date

`dbt run --select fct_reviews`

```text
12:53:32  Loading AIRBNB.DEV.fct_reviews incrementally (all missing dates)
```

`dbt run --select fct_reviews --vars '{start_date: "2024-02-15 00:00:00", end_date: "2024-03-15 23:59:59"}'`

```text
2:54:46  Loading AIRBNB.DEV.fct_reviews incrementally (start_date: 2024-02-15 00:00:00, end_date: 2024-03-15 23:59:59)
```
- review compiled code `dbtlearn/target/compiled/dbtlearn/models/fct/fct_reviews.sql` and it would have the date ranges

```sql
-- code above
WHERE review_text is not null
    AND review_date >= '2024-02-15 00:00:00'
    AND review_date < '2024-03-15 23:59:59'
  
```

- resouce [Configure incremental models](https://docs.getdbt.com/docs/build/incremental-models#about-incremental_strategy)

### Orchestrating dbt with dagster
[Integrate with other orchestration tools](https://docs.getdbt.com/docs/deploy/deployment-tools)
- Airflow
	- large community; many answers out there
	- installation is a bit difficult
	- does not really integrate well with dbt other than building the models
	- manages the whole dbt workflow as a unit
- Prefect
	- more modern ETL tool
	- easy to install; very Pythonic
	- simple integration with dbt
	- doesn't have full-fledge integration although better than Airflow
- Azure Data Factory
	- point and click
	- good product in itself but doesn't really have tight integration with dbt
- dbt Cloud
	- proprietary offering from dbt Labs
- dagster
	- new generation scheduling tool

dagster
- very tight dbt integration; understands dbt project well
- data concept of dagster (asset) is similar to dbt (model)
- great UI
- easy to debug - will see exactly what dagster executed when you execute the workflow

#### dagster installation
- create a `requirements.txt` and `pip install -r requirements.txt` into a virtual environment

```text
dbt-snowflake
dagster-dbt
dagster-webserver
```


- create a dagster project from existing dbt project

`dagster-dbt project scaffold --project-name dbt_dagster_project --dbt-project-dir=dbtlearn`

- before launching dagster, make sure that dbt connections are ok and that the dependencies in dbt have been installed

```bash
cd dbtlearn
dbt debug
dbt deps
```

- add a schedule into the dagster project
`dbt_dagster_project/dbt_dagster_project/schedules.py`, remove the comment in schedules

- start the dagster project

```bash
cd 'dbt_dagster_project'                                    
dagster dev
```

- once server is running, go to `http://127.0.0.1:3000/`

- manage dbt on dagster without touching dbt in production directly
- dagster can also include data integration e.g. airbyte with minimal setup
- can also connect a python based data source (from API)


<center><img src="/assets/images/learning-log/dbt-dagster.png" alt="dagster" width="500"/></center>

Here are some resources about dagster
- [Dagster, dbt, duckdb as new local MDS](https://georgheiler.com/post/dbt-duckdb-production/)
- [Partitioned data pipelines](https://dagster.io/blog/partitioned-data-pipelines)



## 5: 2025-01-30 (Thu)

I've covered these topics today: `dbt-great-expectations`, debugging, and logging

### `dbt-expectations`

[Great Expectations](https://github.com/great-expectations/great_expectations)
- data testing framework
- provides a bunch of functions that can be applied to a data

[dbt-expectations](https://github.com/calogica/dbt-expectations)
- not a port of Great Expectations in dbt but inspired by it
- see [Available tests](https://github.com/calogica/dbt-expectations?tab=readme-ov-file#available-tests)
- to install, add to `project.yml`, and run `dbt deps`

```yaml
packages:
  - package: calogica/dbt_expectations
    version: 0.10.1
```
examples
- comparing row counts between models
  - add to `schema.yml`
  - run `dbt test --select dim_listings_w_hosts` to only run test for this model

```yaml
  - name: dim_listings_w_hosts
    tests:
      - dbt_expectations.expect_table_row_count_to_equal_other_table:
          compare_model: source('airbnb', 'listings')
```

- looking for outliers in your data

```yaml
    columns:
      - name: price
        tests:
        - dbt_expectations.expect_column_quantile_values_to_be_between:
            quantile: .99
            min_value: 50
            max_value: 500
```
- implementing test warnings
  - note that `config` can be set on any tests not just here

```yaml
        - dbt_expectations.expect_column_max_to_be_between:
            max_value: 5000
            config:
              severity: warn
```

- validating column types
  - uses the types of backend (in this case Snowflake)

```yaml
        - dbt_expectations.expect_column_values_to_be_of_type:
            column_type: number
```

- monitoring categorical variables in the source data
  - add to `sources.yml` file
  - to run a test for a source, run `dbt test --select source:airbnb.listings`
    - also runs tests that affect the source table e.g.
      - `dbt_expectations_source_expect_column_distinct_count_to_equal_airbnb_listings_room_type__4 `
      - `dbt_expectations_expect_table_row_count_to_equal_other_table_dim_listings_w_hosts_source_airbnb_listings_`

```yaml
version: 2

sources:
    - name: airbnb
      schema: raw
      tables:
        - name: listings
          identifier: raw_listings
          columns:
            - name: room_type
              tests:
                - dbt_expectations.expect_column_distinct_count_to_equal:
                    value: 4
```
### Debugging
#### Debugging dbt tests
  - `dbt --debug test --select source:airbnb.listings`
    - for debugging, will see all the SQLs that dbt runs against the data warehouse in the terminal screen which can get overwhelming
    - or check the compiled SQL in the target folder `target/compiled/dbtlearn/models/sources.yml/dbt_expectations_source_expect_a60b59a84fbc4577a11df360c50013bb.sql`
      - this can be executed in Snowflake for further debugging 

#### Debugging and working with regular expressions
- e.g. regex for price $90.00
	- `$` and `.` need to be escaped for regex, then escaped for YAML, then need to be escaped twice so the slashes actually render in Snowflake; these happens when there are many interconnected technologies
  - use the compiled SQL command in `targets` directory to debug the regex in Snowflake

```yaml
sources:
    - name: airbnb
      schema: raw
      tables:
        - name: listings
          identifier: raw_listings
          columns:
            - name: price
              tests: 
                - dbt_expectations.expect_column_values_to_match_regex:
                    regex: "^\\\\$[0-9][0-9\\\\.]+$"
```


### Logging
- can log messages to dbt's standard log file, or the terminal, or disable a log message temporarily

#### Logging to the dbt log file
- create a macro `dbtlearn/macros/logging.sql`
- run the macro `dbt run-operation learn_logging`
- this will be logged in the logging file `dbtlearn/logs/dbt.log` as `[debug]`
- can add this Jinja command anywhere e.g. in the middle of a SQL command

```sql
{​% macro learn_logging() %​}
    {​{ log("This is a logging message from the macro") }​}
{​% endmacro %​}​
```

<center><img src="/assets/images/learning-log/dbt-logging-debug.png" alt="dbt-logging-debug" width="500"/></center>


#### Logging to screen
- add a second paramater `info=True`
- message will appear on the screen, and also in the dbt log file as `[info]`

```txt
[0m21:08:00.947064 [info ] [MainThread]: This is a logging message from the macro
```

<center><img src="/assets/images/learning-log/dbt-log-info.png" alt="dbt-log-info" width="500"/></center>

#### Disabling log messages
- there are two layers of execution in dbt
  - when macro is executed, the jinja will be executed
  - then the SQL from Jinja will be executed 
- so to comment out a log, use a Jinja comment tag `#` instead of sql comment tag `--` 
  - ❌ `-- {​{ log("This is a logging message from the macro", info=True) }​}`
  - ✅ `{​# log("This is a logging message from the macro", info=True) #​}`

```sql
{​% macro learn_logging() %​}
   {​# log("This is a logging message from the macro", info=True) #​}
{​% endmacro %​}
```


## 4: 2025-01-29 (Wed)

Today was pretty full. I've covered dbt snapshots, tests, macros, third-party packages, documentation, analyses, hooks, and exposures.

### Snapshots

- Snapshots in dbt implement Type 2 SCD
- Path is as defined in the project YAML file, and consist of SQL files
```yaml
snapshot-paths: ["snapshots]
```
- example code `dbtlearn/snapshots/scd_raw_listings.sql`

```sql
{​% snapshot scd_raw_listings %​}

{​{
    config(
        target_schema = 'dev',
        unique_key='id',
        strategy='timestamp',
        updated_at='updated_at',
        invalidate_hard_deletes = True
    )
}​}

select * from {​{ source('airbnb', 'listings') }​}

{​% endsnapshot %​}
```
- `config` line defines how and where to create the snapshot; in this example it is created as a new table in Snowflake `AIRBNB.DEV.SCD_RAW_LISTINGS`
- `invalidate_hard_deletes = True` means that if the record is deleted, dbt will replace the `dbt_valid_to` date to the snapshot date; if equals to `False`, it will not capture the delete
- the table is materialized with additional columns
- `dbt snapshot` takes the snapshots

<center><img src="/assets/images/learning-log/dbt-snapshot.png" alt="snapshot-table" width="500"/></center>

- as records are updated in the raw table, the SCD table captures the changes as new rows with the `dbt_valid_to` fields updated

<center><img src="/assets/images/learning-log/dbt-scd-table.png" alt="snapshot-table" width="500"/></center>

### Tests
- There are two kinds of tests in the dbt
  - singular
    - SQL queries stored in the  `tests` folder
    - the expected result set is empty
  - generic
    - built-in tests from the core installation of dbt
      - unique
      - not_null
      - accepted_values
      - relationships - takes a column value and makes sure it is a valid reference to records in another table
    - custom tests and tests imported from dbt packages
- `unique` and `not_null` are built in warehouse databases but may not be present in newer types like data lake or lakehouses
- `dbt test` runs the tests


#### Singular tests
- SQL in `tests` folder e.g. `dbtlearn/tests/dim_listings_minimum_nights.sql`
- example

```sql
select
    *
from
    {​{ ref('dim_listings_cleansed') }​}
where minimum_nights < 1 
limit 10
```
- `dbt test --help` brings up the docs
- `dbt test --select dim_listings_cleansed` - runs the tests associated with the `dim_listings_cleansed` model only

#### Generic tests
- define `schema.yml` under `models` folder
  - name does not matter; can be one file or several files
  - can be used for extra configurations, tests, documentation

```yaml
version: 2

models:
  - name: dim_listings_cleansed
    columns:

      - name: listing_id
        tests:
          - unique
          - not_null

      - name: host_id
        tests:
          - not_null
          - relationships:
              to: ref('dim_hosts_cleaned')
              field: host_id

      - name: room_type
        tests:
          - accepted_values:
              values: ['Entire home/apt',
                        'Private room',
                        'Shared room',
                        'Hotel room']
```

#### Custom generic tests
- macros with special signature
- example `dbtlearn/macros/positive_value.sql`
```sql
{​% test positive_value(model, column_name) %​}
select
    *
from
    {​{ model }​}
where
    {​{ column_name }​} < 1
{​% endtest %​}
```
- add these in `schema.yml` under model 

```yaml
version: 2
models:
  - name: dim_listings_cleansed
    columns:

      - <other tests here>
	
      - name: minimum_nights
        tests:
          - positive_value
```

### Macros
- Jinja templates created in the `macros` folder
- can be used in model definitions and tests
  - `test` is a special macro for implementing generic tests
- example: `dbtlearn/macros/no_nulls_in_columns.sql`
  - `-` in the `for` loop means remove whitespaces

```sql
{​% macro no_nulls_in_columns(model) %​}
    select * from {​{ model }​} where
    {​% for col in adapter.get_columns_in_relation(model) -%​}
        {​{ col.column}​} is null or
    {​% endfor %​}
    FALSE
{​% endmacro %​}
```
- use in `dbtlearn/tests/no_nulls_in_dim_listings.sql`
```sql
{​{ no_nulls_in_columns(ref('dim_listings_cleansed')) }​}
```


### Third-party packages
- extend dbt using dbt packages; allows access to a plethora of macros and tests
- go to `hub.getdbt.com`
- example [`dbt-utils`](https://hub.getdbt.com/dbt-labs/dbt_utils/latest/) > [generate_surrogate_key](https://github.com/dbt-labs/dbt-utils/tree/1.3.0/#generate_surrogate_key-source)
  - "cross-database way" if the field values are the same in two or more instances of the database (even if different types of databases), the hash will still be the same

to install:
- create a file in the dbt project root directory called `dbtlearn/packages.yml`

```yaml
packages:
  - package: dbt-labs/dbt_utils
    version: 1.3.0
```

- run `dbt deps` to install
  - might not work behind firewall
  - work around: go to the GitHub repository of the package, copy it manually to the dbt packages folder `dbtlearn/dbt_packages`

- example of usage: `dbtlearn/models/fct/fct_reviews.sql`

```sql
{​{
    config(
        materialized = 'incremental',
        on_schema_change = 'fail'
    )
}​}
with src_reviews as (
    select * from {​{ ref('src_reviews') }​}
)

select 
{​{ dbt_utils.generate_surrogate_key(['listing_id', 'review_date', 'reviewer_name', 'review_text'])}​} as review_id,
* from src_reviews
where review_text is not null

{​% if is_incremental() %​}
    and review_date > (select max(review_date) from {​{this}​})
{​% endif %​}
```

- run `dbt run --select fct_reviews --full-refresh` - specify full refresh so that the run doesn't error out `on_schema_change`
- `review_id` has been added to the table in Snowflake

<center><img src="/assets/images/learning-log/dbt-surrogate-key.png" alt="surrogate-key" width="500"/></center>


### Documentation
- Should be kept close to the actual code as possible
- Defined in two ways
  - in YAML files (e.g. `schema.yml`)
  - in stand alone markdown files
- dbt combines these documentations in an html service that can be used on your own server
- otherwise, dbt ships with a lightweight documentation web server
- `overview.md` is a special file that can be used to customize landing pages
- dbt can also be configured to store assets like downloadables and images in a special folder that can be referenced in a markdown file

#### Basic documentation
- update `schema.yml` to add descriptions

```yaml
version: 2

models:
  - name: dim_listings_cleansed
    description: Cleansed table which contains Airbnb listings.
    columns:

      - name: listing_id
        description: Primary key for the listing
        tests:
          - unique
          - not_null
```
- run `dbt docs generate` to generate docs at `target` folder 
  - catalog will be written to `dbtlearn/target/catalog.json`
  - html in `dbtlearn/target/index.html`
- run `dbt docs serve` to render the documentation in the built-in dbt webserver


<center><img src="/assets/images/learning-log/dbt-docs.png" alt="dbt-docs" width="500"/></center>

#### Markdown-based docs, custom overview page, and assets

- create a markdown file e.g. in the `models` directory `dbtlearn/models/docs.md`

```markdown
{​% docs dim_listing_cleansed__minimum_nights %​}
Minimum number of nights required to rent this property.

Keep in mind that old listings might have `minimum_nights` set to 0 in the source tables. Our cleansing algorithm updates this to `1`.

{​% enddocs %​}
```

- add to description in `schema.yml`

```markdown
       - name: minimum_nights
        description: '{​{ doc("dim_listing_cleansed__minimum_nights") }​}'
        tests:
          - positive_value
```

- can also create an overview doc in `dbtlearn/models/overview.md`, e.g.

```markdown
{​% docs __overview__ %​}
# Airbnb pipeline

Hey, welcome to our Airbnb pipeline documentation!

Here is the schema of our input data:
![input schema](https://dbtlearn.s3.us-east-2.amazonaws.com/input_schema.png)

{​% enddocs %​}
```

- to reference assets locally, add a folder in the project root `assets`
  - make sur to define this path in `project.yml`

```yaml
asset-paths: ["assets"]
```

- replace the url path in the markdown file e.g.

```markdown
{​% docs __overview__ %​}
# Airbnb pipeline

Hey, welcome to our Airbnb pipeline documentation!

Here is the schema of our input data:
![input schema](assets/input_schema.png)

{​% enddocs %​}
```

- running `dbt docs generate` will generate the assets into target folder e.g. `target/assets/input_schema.png`

#### Lineage graph (data flow DAG)
- the lineage graph is accessible in the docs UI

<center><img src="/assets/images/learning-log/dbt-lineage-graph.png" alt="dbt-lineage-graph" width="500"/></center>


- can set filter in the UI e.g.
  - `--select +src_hosts+` 
    - `+` preceeding src_hosts - show dependencies 
    - `+` after src_hosts - everything that builds on src_hosts
  - `--exclude dim_listings_w_hosts+`
    - hide dim_listings_w_hosts and everything that depends on it
- can also use these filters in other dbt commands `dbt run --help` e.g.  
  - e.g. `dbt run --select src_hosts+`
      - src_hosts and every model that builds from src_hosts will execute

### Analyses
- can run a query using dbt syntax and references but not necessarily create a model
- version control, and execute against the data warehouse
- e.g. `dbtlearn/analyses/full_moon_no_sleep.sql`

```sql
with mart_fullmoon_reviews as (
    select * from {​{ ref('mart_fullmoon_reviews') }​}
)

select
    is_full_moon,
    review_sentiment,
    count(*) as reviews
from
    mart_fullmoon_reviews
group by
    is_full_moon,
    review_sentiment
order by
    is_full_moon,
    review_sentiment
```
- run `dbt compile`
- parsed SQL will be written to target e.g. `dbtlearn/target/compiled/dbtlearn/analyses/full_moon_no_sleep.sql` which can be run in the data warehouse

### Hooks
- SQLs that are executed at predefined times
- can be defined at project, models directory, or individual models level
- there are four types of hooks
  - on_run_start: executed at the start of `dbt {run, seed, snapshot}` 
  - on_run_end: executed at the end of `dbt {run, seed, snapshot}`
  - pre-hook: executed before a model/seed/snapshot is built
  - post-hook: executed after a model/seed/snapshot is built

e.g. add a REPORTER role in Snowflake, initially this role does not have access to dbt views and tables
- in project yaml, add hook under models
  - this will run for all models `{​{ this }​}`

```yaml
    +post-hook:
      - "GRANT SELECT ON {​{ this }​} TO ROLE REPORTER"
```

- run `dbt run` to execute the models
- then go to Snowflake to check that the reporter role has select permission on all tables and views from dbt

### Exposure
- configurations that can point to external resources like reports and dashboards
- can be integrated and compiled in documentations

- e.g. create a yaml file `dbtlearn/models/dashboards.yml`

```yaml
version: 2

exposures:
  - name: executive_dashboard
    label: Executive Dashboard
    type: dashboard
    maturity: low
    url: <url to dashboard here>
    description: Executive Dashboard about Airbnb listings and hosts
      

    depends_on:
      - ref('dim_listings_w_hosts')
      - ref('mart_fullmoon_reviews')

    owner:
      name: M
      email: email@example.com
```

- run `dbt docs generate` then `dbt docs serve`
- the exposures also appear in the lineage graph



## 3: 2025-01-26 (Sun)

Today, I've covered dbt seeds, and source.

### Seeds
- local files yet to be uploaded to the warehouse using dbt
- contained in this path `dbtlearn/seeds` e.g. `dbtlearn/seeds/seed_full_moon_dates.csv`
- in the project YAML file, the paths of seeds are defined as

```yaml
seed-paths: ["seeds"]
```
- `dbt seed` command populates seed in Snowflake; this results in a table. (Snowflake can infer the column schema from file)

### Source
- an abstraction layer that sits on the top of the input (raw) tables e.g. those defined in models `/dbtlearn/models/src/src_*.sql`
- contains extra features such as checking for data freshness
- to define models as source, create a separate `source.yml` file under the src model directory `dbtlearn/models/sources.yml`

e.g.
```yaml
version: 2

sources:
    - name: airbnb
      schema: raw
      tables:
        - name: listings
          identifier: raw_listings

        - name: hosts
          identifier: raw_hosts

        - name: reviews
          identifier: raw_reviews
          loaded_at_field: date
          freshness:
            warn_after: {count: 1, period: hour}
            error_after: {count: 24, period: hour}
```

- in `models/src/src_listings.sql` instead of directly referencing the Snowflake table as `AIRBNB.RAW.RAW_LISTINGS`, it can now be abstracted to
    - `{​{ source('airbnb', 'listings') }​}` using names listed in the source YAML file

e.g. 
```sql
with raw_listings as (
select * from {​{ source('airbnb', 'listings') }​}
)

-- continue code here
```

- `dbt compile` checks template tags, and connections; dbt goes through models, YAML files, tests etc and checks if all references are correct

### Source freshness

- in the source YAML file, these lines under `reviews` define freshness
```yaml
          loaded_at_field: date
          freshness:
            warn_after: {count: 1, period: hour}
            error_after: {count: 24, period: hour}
```
- this means
    - give warning if there's no data that's less than an hour fresh
    - return an error if there are no new data in 24 hours

- `dbt source freshness` is the command to use to  check the freshness


### YAML extension recognition in VS Code

My IDE recognizes `*.yaml` extensions as YAML but not `*.yml`. To fix this:
- `CMD + Shift + P`, type "Change language mode" and choose `YAML` language

## 2: 2025-01-25 (Sat)

### dbt models and materialization

Today,  I worked on writing dbt models and materializing them into Snowflake.

<center><img src="/assets/images/learning-log/dbt-project-tables.png" alt="tables-graph" width="500"/></center>

Here's the final outcome in the data warehouse

<center><img src="/assets/images/learning-log/tables-in-wh.png" alt="tables-in-wh" width="200"/></center>

The project YAML file has been modified to follow these defaults; the `+` sign before `+materialized` means that it's a dbt keyword and not a directory
```yml
models:
  dbtlearn:
    +materialized: view
    dim:
      +materialized: table
    src:
      +materialized: ephemeral
```

The following configuration was added for the the two dim tables so they materialized as views instead of tables, overriding the default state in the project YAML.

```sql
{​{
    config(
        materialized = 'view'
    )
}​}
```

Using the dbt plugin was also convenient in that when testing the SQL, I only needed to run it within the editor and it's able to return the results of the query (no need to switch into Snowflake's interface). It can also be used to build the model using GUI.

<center><img src="/assets/images/learning-log/dbt-project-using-plugin.png" alt="tables-graph" width="500"/></center>

`dbt` models are SQL definitions that can be materialized  (or not) into tables or views. These are stored as SQL files under `dbtlearn/models/`. They consist of `select` statements but contain other features such as: semantic dependencies with other models, scripts and macros. 

`dbtprpoject/target/run/dbtlearn/models` - contains the final compiled models. Files here can be investigated when troubleshooting.

CTE (common table expressions) are widely used and recommended as they are readable, easy to maintain, and make complex queries readable. They are similar to views but are not stored

Commands to build models include:
- `dbt run` - materializes the models
- `dbt run --full-refresh` - rebuilds all tables including incremental tables


There are four (4) types of materialization in dbt

type | use | don't use
-| - | - 
view | light weight representation;<br>the model isn't being reused often | the model is being read several times
table | the model is being read repeatedly | single-use models; incremental load
incremental | fact, event tables;<br>appends to tables | update historical records
ephemeral | i.e. no materialization;<br>want to use an alias to a column | model is read several times 


Here's an example of a dbt model using an incremental load, and [Jinja](https://jinja.palletsprojects.com/en/stable/)
```sql
{​{
    config(
        materialized = 'incremental',
        on_schema_change = 'fail'
    )
}​}
with src_reviews as (
    select * from {​{ ref('src_reviews') }​}
)

select * from src_reviews
where review_text is not null

{​% if is_incremental() %​}
    and review_date > (select max(review_date) from {​{this}​})
{​% endif %​}
```

When switching from a materialized model to an ephemeral model, delete the tables manually in the data warehouse as dbt won't delete the tables or views itself.

Here are other things I came across today that are interesting:

### Books for LLM, and Polars
LLM Engineer's Handbook [[Amazon]](https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/) [[Github]](https://github.com/PacktPublishing/LLM-Engineers-Handbook)
- a resource when I get around to exploring LLMs 

Polars Cookbook [[Amazon]](https://www.amazon.com/Polars-Cookbook-practical-transform-manipulate/dp/1805121154/)
- [Polars](https://pola.rs/) is a fast alternative to [Pandas](https://pandas.pydata.org/docs/) specially for large datasets

### pandera

> `pandera` is a Union.ai open source project that provides a flexible and expressive API for performing data validation on dataframe-like objects to make data processing pipelines more readable and robust. Dataframes contain information that pandera explicitly validates at runtime. - [[Docs]](https://pandera.readthedocs.io/en/stable/)

Here's an example from the docs. It's interesting because it seems like a lighter version of Great Expectations wherein the data can be further validated using ranges and other conditions. It's powerful for dataframe validations.

```python
import pandas as pd
import pandera as pa

# data to validate
df = pd.DataFrame({
    "column1": [1, 4, 0, 10, 9],
    "column2": [-1.3, -1.4, -2.9, -10.1, -20.4],
    "column3": ["value_1", "value_2", "value_3", "value_2", "value_1"],
})

# define schema
schema = pa.DataFrameSchema({
    "column1": pa.Column(int, checks=pa.Check.le(10)),
    "column2": pa.Column(float, checks=pa.Check.lt(-1.2)),
    "column3": pa.Column(str, checks=[
        pa.Check.str_startswith("value_"),
        # define custom checks as functions that take a series as input and
        # outputs a boolean or boolean Series
        pa.Check(lambda s: s.str.split("_", expand=True).shape[1] == 2)
    ]),
})

validated_df = schema(df)
print(validated_df)
```

In [Pydantic](https://docs.pydantic.dev/latest/), something like this can also be used

```python
from pydantic import BaseModel, conint, confloat

class Product(BaseModel):
    quantity: conint(ge=1, le=100)  # Validates that quantity is between 1 and 100
    price: confloat(gt=0, le=1000)   # Validates that price is greater than 0 and less than or equal to 1000

product = Product(quantity=10, price=500)
print(product)

```

### Copilot installation, an update
The VS Code plugin that I was trying to install yesterday is working now and I am able to access chat on the side panel as well as see the prompts on screen. Not really sure what fixed it but it could be the reboot of my computer.

## 1: 2025-01-24 (Fri)
### dbt and Snowflake setup
I'm going through the dbt course in Udemy [The Complete dbt (Data Build Tool) Bootcamp: Zero to Hero](https://www.udemy.com/course/complete-dbt-data-build-tool-bootcamp-zero-to-hero-learn-dbt/).[^2] [^3]


[^2]: It was between this and [dbt Learn](https://www.getdbt.com/dbt-learn) platform - I might go back to that for review later. Lecturers worked in Databricks and co-founded dbt Learn so I decided to do this course first - and in the process, subtract from the ever growing number of Udemy courses that I haven't finished :p  
[^3]: This is the repo for the course: [complete-dbt-bootcamp-zero-to-hero](https://github.com/nordquant/complete-dbt-bootcamp-zero-to-hero)

Here's what I did today:
- Created a [Snowflake](https://www.snowflake.com/en/) trial account. It's good for 30 days with $400 credit.
- Setup Snowflake warehouse and permissions for `dbt`, ingested Airbnb data hosted in AWS S3 into the raw tables.
- In my local machine, installed Python 3.12.7 (I needed to downgrade from Python 3.13.0 as [it's not yet supported by dbt](https://docs.getdbt.com/faqs/Core/install-python-compatibility)). I also installed `virtualenv` using Homebrew but I realized I could have just installed it using `pip`. 
- Installed `dbt-snowflake` in the virtual environment (always use a Python virtual environment!)
- Created a dbt user directory `mkdir ~/.dbt`
- Initialized dbt project (`dbt init dbtlearn`) and connected it to the Snowflake account. I also learned just to delete the project directory when deleting a project (I made a configuration mistake! :p)
- Updated  `dbt_project.yml` to remove examples; deleted examples from the project directory
- Installed dbt plugin "Power User for dbt core" in VS Code and set it up.

### Github Copilot
As an aside, I got distracted because the lecturer's VS Code has Copilot enabled so I tried to setup mine. The free version is supposed to be one click and a Github authentication away but for some reason it's buggy in my IDE. Leaving it alone for now. 

## 0: 2024-11-16 (Sat) to 2025-01-08 (Wed) 
### DE Bootcamp
I finished Zach's Free YouTube Data Engineering bootcamp ([DataExport.io](https://www.dataexpert.io)) which started November last year and will run until February 7 (the deadline was extended from last day of January).

The topics covered were:
- Dimensional Data Modeling  
- Fact Data Modeling  
- Apache Spark Fundamentals  
- Applying Analytical Patterns  
- Real-time pipelines with Flink and Kafka  
- Data Visualization and Impact  
- Data Pipeline Maintenance  
- KPIs and Experimentation  
- Data Quality Patterns [^1]

Zach Wilson did a good job of explaining the topics (I'm also very impressed with how well he can explain the labs while writing code without missing a beat). The Data Expert community was also an incredible lot, as some of the setup and homeworks were complicated without prior exposure.

It was a challenging 6 weeks of my life with lectures, labs, and homeworks so much so that there was some lingering emptiness when my schedule freed up as I finished the bootcamp. I'm glad I went through it and it's a good jumping off point for my learning goal this year.

Sharing the link to [my certification](https://bootcamp.techcreator.io/certification/maryletteroa78207/free-bootcamp-completion).

[^1]: This is the repo for the bootcamp and many more: [data-engineering-handbook](https://github.com/DataExpert-io/data-engineer-handbook/)
