---
layout: post
title: 2025 Learning Log
categories: [learning-log]
tags: [data-engineering]
pin: true
---

In keeping up with my goal to do more learning this year, I'm allotting some time outside the daily grind for learning. 

My overarching goal is to explore popular data engineering tools such as `dbt`, and cloud technologies such as Azure Fabric and Snowflake. I haven't worked in Databricks since 2021 so it'll be a good opportunity to re-learn and catch up with the new developments.

Here are some notes and impressions of how the learning is coming along so far, written in reverse chronological order.

For dbt
- I've moved the notes here [dbt-notes]({% link _posts/2025-02-02-dbt-notes.md %})

## 10. 2025-02-05 (Wednesday) - dbt-Fabric, Fireducks

### dbt-Fabric
I was looking around for how to integrate dbt to Azure, and I found these resouces
- [Setup dbt for Fabric Data Warehouse](https://learn.microsoft.com/en-us/fabric/data-warehouse/tutorial-setup-dbt)
    - <i class="fa-brands fa-youtube"></i>[How to run dbt in Microsoft Fabric](https://www.youtube.com/watch?v=Zmh3Z8nhoDw&t=639s)
- Also [dbt cloud is available for Azure Fabric](https://www.getdbt.com/blog/dbt-cloud-is-now-available-for-microsoft-fabric)
    - <i class="fa-brands fa-youtube"></i> [Configure and Create DBT Model for Fabric Warehouse using dbt Cloud ](https://www.youtube.com/watch?v=7UX5qy45c4g)


The process looks straight-forward
- Install the adapter in the virtual environment with Python 3.7 and up `pip install dbt-fabric`
- Make sure to have the [Microsoft ODBC Driver for Sql Server](https://learn.microsoft.com/en-us/sql/connect/odbc/download-odbc-driver-for-sql-server?view=sql-server-ver16#download-for-windows) installed
- Add an existing Fabric warehouse
- The dbt profile in the home directory needs to be setup
- Connect to the Azure warehouse, do the authentication
- Check the connections
- Then the dbt project can be built

Aside from Fabric, dbt also has integrations with other Azure data platforms:
- [Synapse](https://docs.getdbt.com/docs/core/connect-data-platform/azuresynapse-setup)
- [Data Factory](https://learn.microsoft.com/en-us/fabric/data-factory/apache-airflow-jobs-dbt-fabric)
- [SQL Server](https://docs.getdbt.com/docs/core/connect-data-platform/mssql-setup) (SQL Server 2017, SQL Server 2019, SQL Server 2022 and Azure SQL Database)

Here is the docs for other dbt core platform connections
- [dbt Core data platform connections ](https://docs.getdbt.com/docs/core/connect-data-platform/about-core-connections)

### Fireducks
There's another duck in the data space: [Fireducks](https://fireducks-dev.github.io/)
. It's not related to DuckDB, instead it allows existing code using `pandas` to be more performant; it's fully compatible with Pandas API. 

This means there's [zero learning cost](https://medium.com/@fireducks/introduction-to-fireducks-get-performance-beyond-pandas-with-zero-learning-cost-8694d4eab9c6) as all that's needed is to replace `pandas` with `fireducks` and the code should be good to go

```python
# import pandas as pd
import fireducks.pandas as pd
```

or via terminal (no need to change the import)

```bash
python3 -m fireducks.pandas main.py
```

Whereas for `polars`, the code would need to be rewritten; polars code is closer to PySpark.

e.g.
```python
import pandas as pd
import polars as pl
from pyspark.sql.functions import col

# load the file ...
#
# Filter rows where 'column1' is greater than 10
# pandas
filtered_df = df[df['column1'] > 10]
# polars
filtered_df = df.filter(pl.col('column1') > 10
# PySpark
filtered_df = df.filter(col('column1') > 10)
```

Here's a comparison of the performance: [Pandas vs. FireDucks Performance Comparison](https://blog.dailydoseofds.com/p/pandas-vs-fireducks-performance-comparison). It surpassed both `polars` and `pandas` 

## 9: 2025-02-04 (Tuesday) - Snowflake, Duckdb, Isaac Asimov Books

I'm taking a break from learning dbt, and switched focus to learning about databases: Snowflake, and Duckdb. Also, I took a bit of a break so that I can catch up on reading Isaac Asimov's series.

### Snowflake virtual warehouse

I found this free introducton in Udemy [Snowflake Datawarehouse & Cloud Analytics - Introduction](https://www.udemy.com/course/introduction-to-snowflake-cloud-data-warehouse-analytics/). This may not be the best resource out there though as it hasn't been updated. 
However, I learned about provisioning a virtual warehouse in Snowflake with the following example commands


Virtual warehouse
- collection of compute resources (CPUs and allocated memory)
- needed to query data from snowflake, and load data into snowflake
- autoscaling is available in enterprise version but not in standard version

Create a virtual warehouse
```sql
CREATE WAREHOUSE TRAINING_WH
WITH
	WAREHOUSE_SIZE = XSMALL --size
	AUTO_SUSPEND = 60 -- if idle for 60s will be suspended automatically
	AUTO_RESUME = TRUE -- if use executes a query, it will automatically resume on it's own without having to manually restarted the warehouse
	INITIALLY_SUSPEND = TRUE
	STATEMENT_QUEUED_TIMEOUT_IN_SECONDS = 300
	STATEMENT_TIMEOUT_IN_SECONDS = 600;
```

Alternatively, use the user interface as well

Create database

```sql
CREATE DATABASE SALES_DB
DATA_RETENTION_TIME_IN_DAYS = 0
COMMENT = 'Ecommerce sales info';
```

Create schema
```sql
create schema Sales_Data;
create schema Sales_Views;
create schema Sales_Stage;
```

Use the warehouse
```sql
USE WAREHOUSE_TRAINING_WH;
USE DATABASE SALES_DB;
USE SCHEMA Sales_Data;
```

Command to find the current environment
```sql
SELECT CURRENT_DATABASE(), CURRENT_SCHEMA(), CURRENT_WAREHOUSE();
```

Here's a [demo of Snowflake features](https://www.youtube.com/watch?v=9PBvVeCQi0w). I learned that Snowflake has it's own data marketplace, and also has a dashboard feature.

Here are others resources for Snowflake
-  YT channel [@snowflakedevelopers](https://www.youtube.com/@snowflakedevelopers) 
- [Developers website](https://www.snowflake.com/en/developers/)
- [Snowflake documentation](https://docs.snowflake.com/en/index)
- [Snowflake community](https://community.snowflake.com/s/)

### Duckdb overview

Another database that I've been hearing about is [Duckdb](https://duckdb.org/), and I'm honestly very interested in this one as it is light-weight and open-source. It can run in a laptop, and can process GBs of data fast.

It's a file-based database (reminds me of SQLite). It utilizes vectorized execution as opposed to tuple-at a time (row-based database), or column-at-a-time execution (column-based database). It sits somewhere between row-based and column-based in that the data is processed by columns but operates on batches of data at a time. Because of this, it is more memory efficient than column execution (e.g. pandas).


Here is [Gabor Szarnyas' presentation about Duckdb](https://www.youtube.com/watch?v=9Rdwh0rNaf0) which talks in detail about Duckdb capabilities. 


Duckdb isn't a one-to-one comparison with Snowflake, though as it can only scale by memory, and is not distributed (nor with Apache Spark for that matter). It also runs locally. A counterpart to this is [MotherDuck](https://motherduck.com/), which is a cloud data warehousing solution built on top of Duckdb (kind of like [dbt Cloud](https://www.getdbt.com/product/dbt-cloud) to dbt core).

As a side note, I was delighted to learn that DuckDB has a SQL command to exclude columns! (Lol I know but you have no idea how cumbersome it is to write all 20+ columns only to exclude a few :p)

example from [Duckdb Snippets](https://duckdbsnippets.com/snippets/2/using-the-exclude-function-in-duckdb)
```sql
// This will select all information about ducks
// except their height and weight
SELECT * EXCLUDE (height, weight) FROM ducks;
```
Whereas the top [Stack Overflow solution](https://stackoverflow.com/questions/729197/exclude-a-column-using-select-except-columna-from-tablea) for this is 
```sql
/* Get the data into a temp table */
SELECT * INTO #TempTable
FROM YourTable
/* Drop the columns that are not needed */
ALTER TABLE #TempTable
DROP COLUMN ColumnToDrop
/* Get results and drop temp table */
SELECT * FROM #TempTable
DROP TABLE #TempTable
```

I know explicitly writing column names is for "contracts" and exclude isn't very production quality but it would be immensely useful in CTEs where the source columns have already been defined previously.

Okay end of side note :p 

Do you think it's normal to fan over a database? No pun intended. : ))

### Three Laws of Robotics

I'm currently reading Isaac Asimov books. I'm on Book 2 (Foundation and Empire) of the [Foundation Series](https://en.wikipedia.org/wiki/Foundation_(book_series)). I kind of wanted to read the books before I watch Apple TV's Foundation but I realized the series is *totally different* from the books. It was like preparing for an exam only to get entirely out-of-scope questions (which has happened too many times before :p)

Spoiler: <details>Book 1 is logical and clever strategies towards starting a foundation planet, whereas the Apple series is all *pew*, *pew*, guns etc. Not Book 2 though, it's all *pew*, *pew*, space battle :p </details>

Anyway, the [Three Laws of Robotics](https://en.wikipedia.org/wiki/Three_Laws_of_Robotics) (aka Asimov's Laws) didn't originate in the Foundation Books (*I just really want to talk about it* :p ) but in one of his short stories in the [I, Robot](https://en.wikipedia.org/wiki/I,_Robot) collection. 
    
Spoiler: <details>But isn't Demerzel from Apple TV's series inspired by this? Was it a cross-over? But didn't the Robot series and the Foundation Series had cross-overs also?.</details>

The Three Laws of Robotics state:
1. A robot may not injure a human being or, through inaction, allow a human being to come to harm.
2. A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.
3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.

The Three Laws of Robotics originally came from a science-fiction story in the 1940's but it's amazing how forward-looking it is. I'm not really into science-fiction books (I'd rather not mix the two :p) but I'm pulled into Asimov's world nonetheless.



## 8: 2025-02-02 (Sun) - dbt

### dbt - certifications and finished the course
I've finished the course today ðŸŽ‰. 

I've also moved the dbt notes into [another post]({% link _posts/2025-02-02-dbt-notes.md %}) to keep the log tidier.

The last section is an interview about the official dbt certifications. I'm still not sure about doing the certification at this point (I kind of want to get more hands-on time with the tool first) but I like what the interviewee said about doing certifications - you learn a lot more about the tool, faster compared to just using it everyday. For me, I do get a lot of gains studying for certs. If I didn't have those stock knowledge in the first place, it would have been a lot harder to think of other, better ways of approaching a problem. Mostly my qualms about doing certs is how expensive they are! :p


## 7: 2025-02-01 (Sat) - dbt

### dbt - Advance Power user dbt core, introducing dbt to the company

Today isn't as technical as the last couple of days.  I've covered more features of the Power User for dbt core extension powered with AI, and tips on introducing dbt to the company.

The course is wrapping up as well, and I only have one full section left about certifications. 

I learned about 
- Advance Power User dbt core
- Introducing dbt to the company

## 6: 2025-01-31 (Fri) - dbt

### dbt - variables, and dagster

Today I've covered dbt variables and orchestration with dagster. It was my first time setting up dagster. I actually liked dagster because the integration with dbt is tight. I was a bit overwhelmed though with all the coding at the backend to setup the orchestration. It might get easier if I take a deeper look at it. For now, it seems like a good tool to use with dbt. 

## 5: 2025-01-30 (Thu) - dbt

### dbt - great expectations, debugging, and logging

I've covered these topics today: `dbt-great-expectations`, debugging, and logging. 

The dbt-great-expectations package, though not really a port of the Python package, contains many tests that are useful for checking the model. I'm glad `ge` was adapted into dbt as it's also one of the popular data testing tool in Python.

## 4: 2025-01-29 (Wed)  - dbt

### dbt - snapshots, tests, macros, packages, docs, analyses, hooks, exposures.

Today was pretty full. I've covered dbt snapshots, tests, macros, third-party packages, documentation, analyses, hooks, and exposures. Not sure how I completed all of these today but these are pretty important components of dbt. I'm amazed about the documentation support in this tool, and snapshot is another feature in which I say "where has this been all my life?" To think that SCD is that straight-forward in dbt.

## 3: 2025-01-26 (Sun) - dbt

### dbt - seeds, and source

Today, I've covered dbt seeds, and source. At first I thought - why did they need to relabel these CSV files and raw tables? The terminologies were a bit confusing but I guess I should just get used to these.

## 2: 2025-01-25 (Sat) - dbt, books, Python package, Copilot

### dbt - models, and materialization

I've covered dbt models, and materialization. These are pretty core topics in dbt - because dbt is all about models! 

### Books for LLM, and Polars

Here are some materials I came upon today

LLM Engineer's Handbook [[Amazon]](https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/) [[Github]](https://github.com/PacktPublishing/LLM-Engineers-Handbook)
- a resource when I get around to exploring LLMs 

Polars Cookbook [[Amazon]](https://www.amazon.com/Polars-Cookbook-practical-transform-manipulate/dp/1805121154/)
- [Polars](https://pola.rs/) is a fast alternative to [Pandas](https://pandas.pydata.org/docs/) specially for large datasets

### pandera

> `pandera` is a Union.ai open source project that provides a flexible and expressive API for performing data validation on dataframe-like objects to make data processing pipelines more readable and robust. Dataframes contain information that pandera explicitly validates at runtime. - [[Docs]](https://pandera.readthedocs.io/en/stable/)

Here's an example from the docs. It's interesting because it seems like a lighter version of Great Expectations wherein the data can be further validated using ranges and other conditions. It's powerful for dataframe validations.

```python
import pandas as pd
import pandera as pa

# data to validate
df = pd.DataFrame({
    "column1": [1, 4, 0, 10, 9],
    "column2": [-1.3, -1.4, -2.9, -10.1, -20.4],
    "column3": ["value_1", "value_2", "value_3", "value_2", "value_1"],
})

# define schema
schema = pa.DataFrameSchema({
    "column1": pa.Column(int, checks=pa.Check.le(10)),
    "column2": pa.Column(float, checks=pa.Check.lt(-1.2)),
    "column3": pa.Column(str, checks=[
        pa.Check.str_startswith("value_"),
        # define custom checks as functions that take a series as input and
        # outputs a boolean or boolean Series
        pa.Check(lambda s: s.str.split("_", expand=True).shape[1] == 2)
    ]),
})

validated_df = schema(df)
print(validated_df)
```

In [Pydantic](https://docs.pydantic.dev/latest/), something like this can also be used

```python
from pydantic import BaseModel, conint, confloat

class Product(BaseModel):
    quantity: conint(ge=1, le=100)  # Validates that quantity is between 1 and 100
    price: confloat(gt=0, le=1000)   # Validates that price is greater than 0 and less than or equal to 1000

product = Product(quantity=10, price=500)
print(product)

```

### Copilot installation, an update

The VS Code plugin that I was trying to install yesterday is working now and I am able to access chat on the side panel as well as see the prompts on screen. Not really sure what fixed it but it could be the reboot of my computer.

## 1: 2025-01-24 (Fri) - dbt, Copilot

### dbt - introduction ,and setting up

I'm going through the dbt course in Udemy [The Complete dbt (Data Build Tool) Bootcamp: Zero to Hero](https://www.udemy.com/course/complete-dbt-data-build-tool-bootcamp-zero-to-hero-learn-dbt/). I've setup a dbt project and created a Snowflake account.

It was between this and [dbt Learn](https://www.getdbt.com/dbt-learn) platform - I might go back to that for review later. Lecturers worked in Databricks and co-founded dbt Learn so I decided to do this course first - and in the process, subtract from the ever growing number of Udemy courses that I haven't finished :p  

### Github Copilot

As an aside, I got distracted because the lecturer's VS Code has Copilot enabled so I tried to setup mine. The free version is supposed to be one click and a Github authentication away but for some reason it's buggy in my IDE. Leaving it alone for now. 

## 0: 2024-11-16 (Sat) to 2025-01-08 (Wed) - DataExpert Data Engineering Bootcamp
### DE Bootcamp
I finished Zach's Free YouTube Data Engineering bootcamp ([DataExport.io](https://www.dataexpert.io)) which started November last year and will run until February 7 (the deadline was extended from last day of January).

The topics covered were:
- Dimensional Data Modeling  
- Fact Data Modeling  
- Apache Spark Fundamentals  
- Applying Analytical Patterns  
- Real-time pipelines with Flink and Kafka  
- Data Visualization and Impact  
- Data Pipeline Maintenance  
- KPIs and Experimentation  
- Data Quality Patterns

Zach Wilson did a good job of explaining the topics (I'm also very impressed with how well he can explain the labs while writing code without missing a beat). The Data Expert community was also an incredible lot, as some of the setup and homeworks were complicated without prior exposure.

It was a challenging 6 weeks of my life with lectures, labs, and homeworks so much so that there was some lingering emptiness when my schedule freed up as I finished the bootcamp. I'm glad I went through it and it's a good jumping off point for my learning goal this year.

Sharing the link to [my certification](https://bootcamp.techcreator.io/certification/maryletteroa78207/free-bootcamp-completion).
